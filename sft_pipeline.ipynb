{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # access your google drive.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "id": "f0a5d28087039097"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# import os\n",
    "# os.chdir('/content/drive/MyDrive/CSCI 544 Project/Train SFT ') # change to current directory\n",
    "# print(\"Current directory changed to:\", os.getcwd())"
   ],
   "id": "58a8c85860226fc4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T18:47:28.707896Z",
     "start_time": "2024-12-17T18:47:25.650016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!conda activate LLM-RL4Rec-conda-env\n",
    "!source activate LLM-RL4Rec-conda-env"
   ],
   "id": "bdab6755695e87de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "CondaError: Run 'conda init' before 'conda activate'\r\n",
      "\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T18:47:30.453793Z",
     "start_time": "2024-12-17T18:47:28.742474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "!conda activate LLM-RL4Rec-conda-env\n",
    "print(sys.executable)\n"
   ],
   "id": "a10f31a9c8a8f971",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rishabh-project/miniconda3/envs/LLM-RL4Rec-conda-env/bin/python\n",
      "\r\n",
      "CondaError: Run 'conda init' before 'conda activate'\r\n",
      "\r\n",
      "/home/rishabh-project/miniconda3/envs/LLM-RL4Rec-conda-env/bin/python\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T18:47:31.803780Z",
     "start_time": "2024-12-17T18:47:30.593158Z"
    }
   },
   "cell_type": "code",
   "source": "!python --version",
   "id": "dfbbfb4c4dec4ef9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.7\r\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T18:47:34.298252Z",
     "start_time": "2024-12-17T18:47:31.832564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from preprocessing import movies_df\n",
    "!pip install datasets\n",
    "!pip install trl\n",
    "!pip install peft"
   ],
   "id": "ee1bd4083bb07fdf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rishabh-project/PycharmProjects/LLM-RL4Rec\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './dataset/ml-100k/ml-100k/u.data'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpreprocessing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m movies_df\n\u001B[1;32m      2\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39msystem(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpip install datasets\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      3\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39msystem(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpip install trl\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/LLM-RL4Rec/preprocessing.py:8\u001B[0m\n\u001B[1;32m      5\u001B[0m base_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./dataset/ml-100k/ml-100k/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Load the data files\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m ratings_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(base_path \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mu.data\u001B[39m\u001B[38;5;124m'\u001B[39m, sep\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m'\u001B[39m, names\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muser_id\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmovie_id\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrating\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtimestamp\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      9\u001B[0m users_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(base_path \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mu.user\u001B[39m\u001B[38;5;124m'\u001B[39m, sep\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m|\u001B[39m\u001B[38;5;124m'\u001B[39m, names\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muser_id\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mage\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgender\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124moccupation\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzip_code\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     10\u001B[0m movies_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(base_path \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mu.item\u001B[39m\u001B[38;5;124m'\u001B[39m, sep\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m|\u001B[39m\u001B[38;5;124m'\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlatin-1\u001B[39m\u001B[38;5;124m'\u001B[39m, usecols\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m], names\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmovie_id\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtitle\u001B[39m\u001B[38;5;124m'\u001B[39m], header\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[0;32m~/miniconda3/envs/LLM-RL4Rec-conda-env/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m   1014\u001B[0m     dialect,\n\u001B[1;32m   1015\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[1;32m   1023\u001B[0m )\n\u001B[1;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001B[0;32m~/miniconda3/envs/LLM-RL4Rec-conda-env/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    617\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m    619\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[0;32m--> 620\u001B[0m parser \u001B[38;5;241m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[1;32m    622\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[1;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[0;32m~/miniconda3/envs/LLM-RL4Rec-conda-env/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m   1617\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1619\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1620\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_engine(f, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mengine)\n",
      "File \u001B[0;32m~/miniconda3/envs/LLM-RL4Rec-conda-env/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[0;34m(self, f, engine)\u001B[0m\n\u001B[1;32m   1878\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[1;32m   1879\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1880\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m get_handle(\n\u001B[1;32m   1881\u001B[0m     f,\n\u001B[1;32m   1882\u001B[0m     mode,\n\u001B[1;32m   1883\u001B[0m     encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m   1884\u001B[0m     compression\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompression\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m   1885\u001B[0m     memory_map\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmemory_map\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[1;32m   1886\u001B[0m     is_text\u001B[38;5;241m=\u001B[39mis_text,\n\u001B[1;32m   1887\u001B[0m     errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding_errors\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstrict\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m   1888\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstorage_options\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m   1889\u001B[0m )\n\u001B[1;32m   1890\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1891\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[0;32m~/miniconda3/envs/LLM-RL4Rec-conda-env/lib/python3.12/site-packages/pandas/io/common.py:873\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    868\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    869\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[1;32m    870\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[1;32m    871\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[1;32m    872\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[0;32m--> 873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(\n\u001B[1;32m    874\u001B[0m             handle,\n\u001B[1;32m    875\u001B[0m             ioargs\u001B[38;5;241m.\u001B[39mmode,\n\u001B[1;32m    876\u001B[0m             encoding\u001B[38;5;241m=\u001B[39mioargs\u001B[38;5;241m.\u001B[39mencoding,\n\u001B[1;32m    877\u001B[0m             errors\u001B[38;5;241m=\u001B[39merrors,\n\u001B[1;32m    878\u001B[0m             newline\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    879\u001B[0m         )\n\u001B[1;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m    882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './dataset/ml-100k/ml-100k/u.data'"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Environments\n",
    "\n",
    " - HF_TOKEN = \"your hugging face API key\", set at notebook secret.\n",
    " - Download packages:\n",
    "  - trl\n",
    "  - datasets\n",
    "  - peft\n",
    " - Access to your google drive."
   ],
   "id": "be43ca7651b881c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Datasets",
   "id": "4a2c9b8e00bb2ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T18:48:23.249590Z",
     "start_time": "2024-12-17T18:48:20.634070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import load_dataset"
   ],
   "id": "58bf047ee9a30590",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishabh-project/miniconda3/envs/LLM-RL4Rec-conda-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T18:48:24.109737Z",
     "start_time": "2024-12-17T18:48:24.104649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DO NOT RUN. example open source datasets, used to rule out issues within trl.\n",
    "#dataset = load_dataset(\"stanfordnlp/imdb\", split=\"train\")"
   ],
   "id": "adf67f0561fcb8b1",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T19:23:16.655794Z",
     "start_time": "2024-12-17T19:23:16.627057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "movies_df = pd.read_csv('dataset/ml-100k/u.item', sep='|', encoding='latin-1',\n",
    "                                         names=['movie_id', 'movie_title', 'release_date', 'video_release_date',\n",
    "                                                'imdb_url', 'unknown', 'action',\n",
    "                                                'adventure', 'animation', 'childrens', 'comedy', 'crime', 'documentary',\n",
    "                                                'drama', 'fantasy',\n",
    "                                                'film_noir', 'horror', 'musical', 'mystery', 'romance', 'sci_fi',\n",
    "                                                'thriller', 'war', 'western'])\n",
    "# movies_df = pd.read_csv('dataset/ml-1m/movies.csv')\n",
    "# movies_df.head()"
   ],
   "id": "32832d2688ee87b",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T19:23:25.755570Z",
     "start_time": "2024-12-17T19:23:25.746748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_movies = list(movies_df.movie_title.values)\n",
    "all_movies[:3]"
   ],
   "id": "ad176a684cb6e491",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Toy Story (1995)', 'GoldenEye (1995)', 'Four Rooms (1995)']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T18:48:27.650712Z",
     "start_time": "2024-12-17T18:48:27.641830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_prompt(attributes, include_dislikes=True, top_k=5):\n",
    "        # Build the dynamic prompt with both liked and disliked movies\n",
    "    age = attributes['age']\n",
    "    gender = attributes['gender']\n",
    "    location = attributes['location']\n",
    "    occupation = attributes['occupation']\n",
    "    liked_movies = attributes['liked_movies']\n",
    "    disliked_movies = attributes['disliked_movies']\n",
    "    prompt = f\"\"\"I am a {gender}, aged {age}, from {location}, working as {occupation}.\"\"\"\n",
    "\n",
    "    if liked_movies:\n",
    "        prompt += f\"\"\"I have previously watched and liked the movies: {liked_movies}.\"\"\"\n",
    "\n",
    "    if include_dislikes and disliked_movies:\n",
    "        prompt += f\"\\nI have watched and HATED the movies: {disliked_movies}.\"\n",
    "\n",
    "    prompt += f\"\"\"Please provide recommendations for movies, based on my history.\n",
    "Based on my profile, recommend the top {top_k} movies I am most likely to watch next.\n",
    "Please provide the output in a list of strings format, containing only the movie titles.\n",
    "Make sure to strictly adhere to the output format given below. Strictly do not generate any additional information other than the movie names.\n",
    "Format:  ['movie_name', 'movie_name', ... 'movie_name']\n",
    "Make sure to limit the recommendations to movies available in the MovieLens dataset.\"\"\"\n",
    "\n",
    "    return prompt"
   ],
   "id": "1b56e09ec495bd89",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T19:24:36.974325Z",
     "start_time": "2024-12-17T19:24:36.893491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# OUR DATA.\n",
    "# Step 1: Load the JSON file into the Colab notebook\n",
    "# data_path = \"./data-1m.json\"  # Update this path if necessary\n",
    "data_path = \"./data.json\"\n",
    "with open(data_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Convert the JSON data into a list of dictionaries\n",
    "formatted_data = [\n",
    "    {\"prompt\": entry[\"prompt\"], \"completion\": \", \".join(entry[\"ground_truth\"])}\n",
    "    for entry in data.values()\n",
    "]\n",
    "# formatted_data = [\n",
    "#     {\"prompt\": generate_prompt(entry[\"attributes\"]), \"completion\": \", \".join(entry[\"ground_truth\"])}\n",
    "#     for entry in data.values()\n",
    "# ]\n",
    "\n",
    "# Step 2: Perform train-validation-test split, 70-15-15\n",
    "train_data, temp_data = train_test_split(formatted_data, test_size=0.3, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert the splits into Hugging Face datasets\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "# Combine the datasets into a DatasetDict for better organization\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "# Step 3: Prepare data for SFTTrainer\n",
    "# SFTTrainer expects the dataset to have 'prompt' and 'completion' keys\n",
    "print(\"3 sample data from training set:\")\n",
    "for i in range(3):\n",
    "  print(dataset[\"train\"][i])\n",
    "\n",
    "print(\"Train data size:\", dataset[\"train\"].num_rows, \"row.\")\n",
    "print(\"Validation data size:\", dataset[\"validation\"].num_rows, \"row.\")\n",
    "print(\"Test data size:\", dataset[\"test\"].num_rows, \"row.\")"
   ],
   "id": "de26d0b0a4b05c9a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 sample data from training set:\n",
      "{'prompt': \"I am a male, aged 19, from Charlottesville, VA, working as student.I have previously watched and liked the movies: ['Truth About Cats & Dogs, The (1996) (comedy, romance)', 'Birdcage, The (1996) (comedy)', 'Jerry Maguire (1996) (drama, romance)', 'Twelve Monkeys (1995) (drama, sci_fi)'].\\nI have watched and HATED the movies: ['Multiplicity (1996) (comedy)', 'Time to Kill, A (1996) (drama)', 'Down Periscope (1996) (comedy)', 'Twister (1996) (action, adventure, thriller)', 'Ransom (1996) (drama, thriller)'].Please provide recommendations for movies released before April 22nd, 1998, based on my history.\\nBased on my profile, recommend the top 5 movies I am most likely to watch next.\\nPlease provide the output in a list of strings format, containing only the movie titles.\\nMake sure to strictly adhere to the output format given below. Strictly do not generate any additional information other than the movie names.\\nFormat:  ['movie_name', 'movie_name', ... 'movie_name']\\nMake sure to limit the recommendations to movies available in the MovieLens dataset.\", 'completion': 'Mission: Impossible (1996), Jack (1996), 12 Angry Men (1957)'}\n",
      "{'prompt': \"I am a male, aged 26, from Memphis, TN, working as writer.I have previously watched and liked the movies: ['Jackie Brown (1997) (crime, drama)', 'Keys to Tulsa (1997) (crime)', 'Gattaca (1997) (drama, sci_fi, thriller)', 'Mrs. Dalloway (1997) (romance)', 'English Patient, The (1996) (drama, romance, war)'].\\nI have watched and HATED the movies: ['Lost Highway (1997) (mystery)', 'Game, The (1997) (mystery, thriller)', 'I Know What You Did Last Summer (1997) (horror, mystery, thriller)', 'Scream 2 (1997) (horror, thriller)'].Please provide recommendations for movies released before April 22nd, 1998, based on my history.\\nBased on my profile, recommend the top 5 movies I am most likely to watch next.\\nPlease provide the output in a list of strings format, containing only the movie titles.\\nMake sure to strictly adhere to the output format given below. Strictly do not generate any additional information other than the movie names.\\nFormat:  ['movie_name', 'movie_name', ... 'movie_name']\\nMake sure to limit the recommendations to movies available in the MovieLens dataset.\", 'completion': \"Afterglow (1997), Boogie Nights (1997), Scream (1996), Devil's Advocate, The (1997), Alien: Resurrection (1997)\"}\n",
      "{'prompt': \"I am a male, aged 35, from Pascoag, RI, working as student.I have previously watched and liked the movies: ['Dead Presidents (1995) (action, crime, drama)', 'Panther (1995) (drama)', 'Cliffhanger (1993) (action, adventure, crime)', 'Operation Dumbo Drop (1995) (action, adventure, comedy, war)', 'Junior (1994) (comedy, sci_fi)'].\\nI have watched and HATED the movies: ['Doors, The (1991) (drama, musical)', 'Glengarry Glen Ross (1992) (drama)', 'Die Hard: With a Vengeance (1995) (action, thriller)'].Please provide recommendations for movies released before April 22nd, 1998, based on my history.\\nBased on my profile, recommend the top 5 movies I am most likely to watch next.\\nPlease provide the output in a list of strings format, containing only the movie titles.\\nMake sure to strictly adhere to the output format given below. Strictly do not generate any additional information other than the movie names.\\nFormat:  ['movie_name', 'movie_name', ... 'movie_name']\\nMake sure to limit the recommendations to movies available in the MovieLens dataset.\", 'completion': \"Jason's Lyric (1994), Angels in the Outfield (1994), Legends of the Fall (1994), Star Trek: The Motion Picture (1979), Pretty Woman (1990)\"}\n",
      "Train data size: 611 row.\n",
      "Validation data size: 131 row.\n",
      "Test data size: 132 row.\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Training\n",
    "- SFTConfig\n",
    "  - TrainingArguments\n",
    "- SFTTrainer\n",
    "- LoraConfig\n",
    "  - Qlora\n",
    "\n",
    "- Reference:\n",
    "  - [tutorial_1](https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/fine-tune-llms-in-2024-with-trl.ipynb)\n",
    "  -\n",
    "\n",
    "\n",
    "\n",
    "#### TODO\n",
    "- No access to llama-3.2"
   ],
   "id": "b3a8ac6881d88db8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Shared setting",
   "id": "a6707d345e931c2b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T18:48:43.569924Z",
     "start_time": "2024-12-17T18:48:38.165266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from peft import LoraConfig\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from random import randint\n",
    "from tqdm import tqdm"
   ],
   "id": "5b9a3540c7692161",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load Model with PEFT adapter\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=None,  # Avoid auto device mapping\n",
    ").to(\"cuda\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# load into pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=\"cuda\")"
   ],
   "id": "273ea6b73c080d06"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T11:16:54.732821Z",
     "start_time": "2024-12-15T11:16:54.727282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# model\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\" # meta-llama/Llama-3.2-11B-Vision-Instruct"
   ],
   "id": "f1283df5c6c57e7c",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T06:50:58.989028Z",
     "start_time": "2024-12-15T06:50:56.379296Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install huggingface_hub",
   "id": "8e9a4ea2241c3892",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /home/rishabh-project/miniconda3/envs/LLM-RL4Rec-conda-env/lib/python3.12/site-packages (0.26.3)\r\n",
      "Requirement already satisfied: filelock in /home/rishabh-project/miniconda3/envs/LLM-RL4Rec-conda-env/lib/python3.12/site-packages (from huggingface_hub) (3.16.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/rishabh-project/miniconda3/envs/LLM-RL4Rec-conda-env/lib/python3.12/site-packages (from huggingface_hub) (2024.9.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /home/rishabh-project/miniconda3/envs/LLM-RL4Rec-conda-env/lib/python3.12/site-packages (from huggingface_hub) (24.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/rishabh-project/miniconda3/envs/LLM-RL4Rec-conda-env/lib/python3.12/site-packages (from huggingface_hub) (6.0.2)\r\n",
      "Requirement already satisfied: requests in /home/rishabh-project/miniconda3/envs/LLM-RL4Rec-conda-env/lib/python3.12/site-packages (from huggingface_hub) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/rishabh-project/miniconda3/envs/LLM-RL4Rec-conda-env/lib/python3.12/site-packages (from huggingface_hub) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/rishabh-project/miniconda3/envs/LLM-RL4Rec-conda-env/lib/python3.12/site-packages (from huggingface_hub) (4.11.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/rishabh-project/miniconda3/envs/LLM-RL4Rec-conda-env/lib/python3.12/site-packages (from requests->huggingface_hub) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/rishabh-project/miniconda3/envs/LLM-RL4Rec-conda-env/lib/python3.12/site-packages (from requests->huggingface_hub) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/rishabh-project/miniconda3/envs/LLM-RL4Rec-conda-env/lib/python3.12/site-packages (from requests->huggingface_hub) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/rishabh-project/miniconda3/envs/LLM-RL4Rec-conda-env/lib/python3.12/site-packages (from requests->huggingface_hub) (2024.8.30)\r\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T18:48:43.697002Z",
     "start_time": "2024-12-17T18:48:43.580366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(new_session=False,\n",
    "      write_permission=False,\n",
    "      token='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',\n",
    "      add_to_git_credential=False)"
   ],
   "id": "21b148b1e7baabd2",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T17:34:20.131614Z",
     "start_time": "2024-12-15T17:34:19.934440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ],
   "id": "ceb6730ee82f714d",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m pipe \u001B[38;5;241m=\u001B[39m pipeline(\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext-generation\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m----> 3\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel_id,\n\u001B[1;32m      4\u001B[0m     torch_dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mbfloat16,\n\u001B[1;32m      5\u001B[0m     device_map\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      6\u001B[0m )\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model_id' is not defined"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Test on sample\n",
    "idx = 30\n",
    "prompt = dataset['validation'][idx]['prompt']\n",
    "message = [{\"role\": \"user\", \"content\": prompt}]\n",
    "outputs = pipe(message, max_new_tokens=256)\n",
    "\n",
    "# print(f\"Query:\\n{dataset['validation'][idx]['prompt']}\")\n",
    "# print(f\"Original Answer:\\n{dataset['validation'][idx]['completion']}\")\n",
    "# #print(f\"Generated Answer:\\n{outputs}\")"
   ],
   "id": "ec877dfc2ef95286"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# import re\n",
    "#\n",
    "# def extract_movie_titles(llm_output):\n",
    "#     \"\"\"\n",
    "#     Extracts movie titles from LLM output formatted as a Python list of strings.\n",
    "#\n",
    "#     Args:\n",
    "#     llm_output: The LLM output string.\n",
    "#\n",
    "#     Returns:\n",
    "#     A list of movie titles.\n",
    "#     \"\"\"\n",
    "#     match = re.search(r\"\\[(?:'([^']*)'(?:,\\s*)?)*\\]\", llm_output)\n",
    "#     if match:\n",
    "#         movie_titles_string = match.group(0)\n",
    "#         # Split by either ', ' or by '[' and ']' and then filter out empty strings\n",
    "#         movie_titles = [title.strip(\" '\") for title in re.split(r\"',\\s*|'|\\[|\\]\", movie_titles_string) if title.strip(\" '\")]\n",
    "#         return movie_titles\n",
    "#     else:\n",
    "#         return []"
   ],
   "id": "f2e2e6871cf345da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T18:48:49.529904Z",
     "start_time": "2024-12-17T18:48:49.523212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "def extract_movie_titles(llm_output):\n",
    "    \"\"\"\n",
    "    Extracts movie titles from LLM output, even if the list is incomplete due to token limit.\n",
    "\n",
    "    Args:\n",
    "    llm_output: The LLM output string.\n",
    "\n",
    "    Returns:\n",
    "    A list of up to 5 movie titles.\n",
    "    \"\"\"\n",
    "    # Match a list-like structure starting with '[' and extract the contents\n",
    "    match = re.search(r\"\\[.*\", llm_output)\n",
    "    if match:\n",
    "        # Extract the matched portion starting from '['\n",
    "        movie_titles_string = match.group(0)\n",
    "        # Split by commas, considering possible quotes and whitespace\n",
    "        movie_titles = re.findall(r\"'(.*?)'|\\\"(.*?)\\\"\", movie_titles_string)\n",
    "        # Flatten the tuple results from re.findall\n",
    "        movie_titles = [title for pair in movie_titles for title in pair if title]\n",
    "\n",
    "        # If no valid quotes are found (e.g., incomplete or malformed list), attempt to split by commas\n",
    "        if not movie_titles:\n",
    "            movie_titles = [title.strip() for title in movie_titles_string[1:].split(\",\")]\n",
    "\n",
    "        # Return up to the first 5 titles\n",
    "        return movie_titles[:5]\n",
    "    return []\n"
   ],
   "id": "98ca43aef0a2ca59",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# eval(outputs[0]['generated_text'][-1]['content'])\n",
    "llm_generated_movies = extract_movie_titles(outputs[0]['generated_text'][-1]['content'])\n",
    "llm_generated_movies\n"
   ],
   "id": "341d20ce76280dae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "llm_generated_movies[1]",
   "id": "88b1fea16b71e409"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T06:55:32.553775Z",
     "start_time": "2024-12-15T06:55:29.980168Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install rapidfuzz",
   "id": "3d3fbf08e50718f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rapidfuzz in /home/rishabh-project/miniconda3/envs/LLM-RL4Rec-conda-env/lib/python3.12/site-packages (3.10.1)\r\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T18:48:53.806379Z",
     "start_time": "2024-12-17T18:48:53.728663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "def match_titles_batch(llm_generated_movies, all_movies):\n",
    "    \"\"\"\n",
    "    Performs fuzzy matching between two lists of movie titles.\n",
    "\n",
    "    Args:\n",
    "        llm_generated_movies: A list of movie titles generated by the LLM.\n",
    "        all_movies: A list of all available movie titles.\n",
    "\n",
    "    Returns:\n",
    "        A list of (matched_title, match_score) tuples.\n",
    "    \"\"\"\n",
    "    matched_titles_with_scores = []\n",
    "    for output in llm_generated_movies:\n",
    "        result = process.extractOne(\n",
    "            query=output,\n",
    "            choices=all_movies,\n",
    "            scorer=fuzz.WRatio\n",
    "        )\n",
    "        if result:  # If a match is found\n",
    "            matched_titles_with_scores.append((result[0], result[1]))  # Append tuple (title, score)\n",
    "        else:\n",
    "            matched_titles_with_scores.append((None, 0))  # Append (None, 0) for no match\n",
    "\n",
    "    return matched_titles_with_scores\n",
    "\n",
    "# Example usage:\n",
    "llm_generated_movies = [\"matrix\", \"inception movie\", \"god father\", \"unknown movie\"]\n",
    "all_movies_1 = [\"The Matrix\", \"Inception\", \"Avatar\", \"Titanic\", \"The Godfather\"]\n",
    "\n",
    "matched_titles_with_scores = match_titles_batch(llm_generated_movies, all_movies_1)\n",
    "print(\"LLM Outputs:\", llm_generated_movies)\n",
    "print(\"Matched Titles with Scores:\", matched_titles_with_scores)"
   ],
   "id": "52883ed917cdb815",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Outputs: ['matrix', 'inception movie', 'god father', 'unknown movie']\n",
      "Matched Titles with Scores: [('The Matrix', 81.81818181818181), ('Inception', 80.0), ('The Godfather', 69.56521739130434), ('Inception', 27.27272727272727)]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T07:07:35.291596Z",
     "start_time": "2024-12-15T07:07:35.180889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training arguments, see reference.\n",
    "training_args = SFTConfig(                            # inherits TrainingArguments class.\n",
    "    max_seq_length=512,                               # max sequence length of the input data.\n",
    "    output_dir=\"./models/\",        # directory to save and repository id\n",
    "    num_train_epochs=3,                     # number of training epochs\n",
    "    learning_rate=5e-5,                     # learning rate\n",
    "    per_device_train_batch_size=1,          # batch size per device during training\n",
    "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
    "    logging_dir=\"./logs\",                   # Directory for logging, enable prettier log print\n",
    "    logging_steps=20,                         # Log after every step\n",
    "    report_to=[],                            # Turn off wandb\n",
    "    fp16=True,                                #\n",
    "\n",
    "    # bf16=True,                              # use bfloat16 precision\n",
    "    # lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
    "    # warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
    "\n",
    "    # tf32=True,                              # use tf32 precision\n",
    "    # push_to_hub=True,                       # push model to hub\n",
    "    # report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    "    gradient_accumulation_steps=8,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True            # use gradient checkpointing to save memory\n",
    "    # optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    # logging_steps=10,                       # log every 10 steps\n",
    "    # max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    ")"
   ],
   "id": "dd7a5c66cbe413c4",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T18:48:59.929359Z",
     "start_time": "2024-12-17T18:48:59.922188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['completion'])):\n",
    "        text = f\"### Prompt: {example['prompt'][i]}\\n ### Completion: {example['completion'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts"
   ],
   "id": "8a6a3a9f687d4a64",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T18:49:00.446971Z",
     "start_time": "2024-12-17T18:49:00.439780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# adding\n",
    "def role_formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['completion'])):\n",
    "        text = f\"### role: user, content: {example['prompt'][i]}\\n ### role: assistant, content: [{example['completion'][i]}]\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts"
   ],
   "id": "2540aad2194fe99c",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CHANGE TO YOUR OWN PATH\n",
    "name = input(\"Enter your name: \")\n",
    "path = f\"/content/drive/MyDrive/CSCI 544 Project/Train SFT /models/{name}/\""
   ],
   "id": "2ebcecf979f85d48"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Standard SFT",
   "id": "6a10880834808922"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# load pretrained model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)          # model for sft"
   ],
   "id": "ab2ca539572d0b37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define trainer.\n",
    "trainer = SFTTrainer(\n",
    "    model=model,                    # seperate model from\n",
    "    train_dataset=dataset['train'], # .select(range(20)\n",
    "    args=training_args,\n",
    "    formatting_func=formatting_prompts_func\n",
    ")"
   ],
   "id": "a5ce3377bf94194c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer.train()",
   "id": "468b77ea8f03783b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer.save_model(path + \"stf-std-gpt2-full-train-data\")",
   "id": "df298aa1250448be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# test for 20 train samples.\n",
    "trainer.train()"
   ],
   "id": "8a81b34a05c79e7a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## SFT with QLora",
   "id": "b71e2bcd12e9177b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T07:07:43.516078Z",
     "start_time": "2024-12-15T07:07:42.289724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load pretrained model\n",
    "model_lora = AutoModelForCausalLM.from_pretrained(model_id)    # model for lora"
   ],
   "id": "6d62ea3f99e644ae",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# save_directory = \"/content/drive/MyDrive/CSCI 544 Project/Train SFT /models/meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# # Save the model\n",
    "# model_lora.save_pretrained(save_directory)"
   ],
   "id": "f592e6550beb9a0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "eed767e901bcc23e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T07:07:53.898537Z",
     "start_time": "2024-12-15T07:07:53.892304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# QLoRA config\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=128,\n",
    "        lora_dropout=0.05,\n",
    "        r=256,\n",
    "        bias=\"none\",\n",
    "        target_modules=\"all-linear\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")"
   ],
   "id": "86c9c9b14f99cf63",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T07:08:13.674776Z",
     "start_time": "2024-12-15T07:07:55.750657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Qlora_trainer\n",
    "lora_trainer = SFTTrainer(\n",
    "    model=model_lora,\n",
    "    train_dataset=dataset['train'], # .select(range(20)\n",
    "    peft_config=peft_config,\n",
    "    args=training_args,\n",
    "    formatting_func=role_formatting_prompts_func\n",
    ")"
   ],
   "id": "cd897e74eb7933d6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4030/4030 [00:01<00:00, 3235.17 examples/s]\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T10:12:53.674784Z",
     "start_time": "2024-12-15T07:08:46.388474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# full train data, 611 row.\n",
    "lora_trainer.train()"
   ],
   "id": "e2d67cca721ac562",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1509' max='1509' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1509/1509 3:03:58, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.085200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.521700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.463000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.430600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.416300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.404300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.400200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.391000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.384900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.383300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.371900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.376100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.375300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.361000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.357900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.366700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.354000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.355200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.351100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.348700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.350900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.337000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.338500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.335500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.344800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.327700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.327100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.320100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.323900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.319200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.321900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.322600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.325700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.314500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.314600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.321300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.315600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.316500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.322000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.314600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.312100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.320600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.314300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.313500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.316400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.312800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.312800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.307200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.299100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.297900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.298600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.301800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.302600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.302400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.298600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.295600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.293000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.293600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.303800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.298200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.290200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.303700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.294600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.295000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.299600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.304100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.288100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.295000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.294600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.289300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>0.295400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>0.298000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.301100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishabh-project/miniconda3/envs/LLM-RL4Rec-conda-env/lib/python3.12/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 07ad0667-0a2b-4384-a6d9-04b484e843c8)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/rishabh-project/miniconda3/envs/LLM-RL4Rec-conda-env/lib/python3.12/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1509, training_loss=0.34284060802421795, metrics={'train_runtime': 11046.9868, 'train_samples_per_second': 1.094, 'train_steps_per_second': 0.137, 'total_flos': 2.4514580896751616e+16, 'train_loss': 0.34284060802421795, 'epoch': 2.995533498759305})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T11:11:49.349853Z",
     "start_time": "2024-12-15T11:11:45.842967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save model\n",
    "lora_trainer.save_model(\"/home/rishabh-project/PycharmProjects/LLM-RL4Rec/models/sft-1m\")"
   ],
   "id": "a335bf70a6e81a95",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T11:07:44.032897Z",
     "start_time": "2024-12-15T11:07:44.028933Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "2653bab164c8be5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Evaluation\n",
    "\n"
   ],
   "id": "5b058f2814668f9f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T18:49:14.106993Z",
     "start_time": "2024-12-17T18:49:14.101275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from random import randint\n",
    "from tqdm import tqdm"
   ],
   "id": "3611dcaf2bdfe4c9",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T18:49:14.692566Z",
     "start_time": "2024-12-17T18:49:14.687234Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.empty_cache()",
   "id": "6f18e57f752691bf",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T17:27:50.510904Z",
     "start_time": "2024-12-15T17:27:50.505599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# change model name (after merged) to which ever you wanna try\n",
    "test_model_id = \"./models/sft-1m\""
   ],
   "id": "9534af37305be8b6",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T17:28:23.101341Z",
     "start_time": "2024-12-15T17:28:07.965725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load Model with PEFT adapter\n",
    "test_model = AutoModelForCausalLM.from_pretrained(\n",
    "    test_model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=None,  # Avoid auto device mapping\n",
    ").to(\"cuda\")\n",
    "\n",
    "test_tokenizer = AutoTokenizer.from_pretrained(test_model_id)\n",
    "\n",
    "# load into pipeline\n",
    "test_pipe = pipeline(\"text-generation\", model=test_model, tokenizer=test_tokenizer)"
   ],
   "id": "5722a5e64792e1eb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# pipe = pipeline(\"text-generation\", model=model_lora, tokenizer=tokenizer)"
   ],
   "id": "e43f73df4b97b39c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=test_model_id,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",\n",
    "# )"
   ],
   "id": "b7bf05135d648a92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load our test dataset\n",
    "#eval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\n",
    "\n",
    "# Test on sample\n",
    "idx = 8\n",
    "prompt = dataset['validation'][idx]['prompt']\n",
    "message = [{\"role\": \"user\", \"content\": prompt}]\n",
    "outputs = test_pipe(message, max_new_tokens=256)\n",
    "\n",
    "# print(f\"Query:\\n{dataset['validation'][idx]['prompt']}\")\n",
    "# print(f\"Original Answer:\\n{dataset['validation'][idx]['completion']}\")\n",
    "# print(f\"Generated Answer:\\n{outputs}\")\n",
    "print(f\"Generated Answer:\\n{outputs[0]['generated_text']}\")"
   ],
   "id": "279a3dcf752729df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "llm_generated_movies = extract_movie_titles(outputs[0]['generated_text'][-1]['content'])\n",
    "llm_generated_movies"
   ],
   "id": "31fe33264c6e7396"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "matched_titles_with_scores = match_titles_batch(llm_generated_movies, all_movies)\n",
    "print(\"LLM Outputs:\", llm_generated_movies)\n",
    "print(\"Matched Titles with Scores:\", matched_titles_with_scores)"
   ],
   "id": "e10c50b889d9a470"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T18:49:26.673429Z",
     "start_time": "2024-12-17T18:49:26.662266Z"
    }
   },
   "cell_type": "code",
   "source": "from baseline.baseline_LLM.evaluation import evaluate_recommendations",
   "id": "1f3a59f414ebf7ab",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T19:25:16.458450Z",
     "start_time": "2024-12-17T19:25:16.449787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_set = dataset['test']\n",
    "test_set[0]"
   ],
   "id": "732c7a329872acaf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'I am a male, aged 18, from Beverly Hills, CA, working as student.I have previously watched and liked the movies: [\\'Monty Python and the Holy Grail (1974) (comedy)\\', \\'Raging Bull (1980) (drama)\\', \\'Terminator 2: Judgment Day (1991) (action, sci_fi, thriller)\\', \"One Flew Over the Cuckoo\\'s Nest (1975) (drama)\", \\'GoodFellas (1990) (crime, drama)\\'].Please provide recommendations for movies released before April 22nd, 1998, based on my history.\\nBased on my profile, recommend the top 5 movies I am most likely to watch next.\\nPlease provide the output in a list of strings format, containing only the movie titles.\\nMake sure to strictly adhere to the output format given below. Strictly do not generate any additional information other than the movie names.\\nFormat:  [\\'movie_name\\', \\'movie_name\\', ... \\'movie_name\\']\\nMake sure to limit the recommendations to movies available in the MovieLens dataset.',\n",
       " 'completion': 'Aliens (1986), Fugitive, The (1993), Die Hard 2 (1990), Blues Brothers, The (1980), When Harry Met Sally... (1989)'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T19:25:19.390824Z",
     "start_time": "2024-12-17T19:25:19.381773Z"
    }
   },
   "cell_type": "code",
   "source": "test_set_reduced = test_set.select(range(10))",
   "id": "eeb8c43e14e068ae",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T19:25:20.673785Z",
     "start_time": "2024-12-17T19:25:20.668641Z"
    }
   },
   "cell_type": "code",
   "source": "import logging",
   "id": "1387bc5476600121",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T19:25:21.946185Z",
     "start_time": "2024-12-17T19:25:21.936713Z"
    }
   },
   "cell_type": "code",
   "source": "test_set_reduced",
   "id": "dc28db438639810",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'completion'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# def evaluate_model(model_id, k=5):\n",
    "#     # Configure logging dynamically based on model_id\n",
    "#     log_filename = f\"{model_id}_evaluation.log\"  # Log file name specific to the model\n",
    "#     logging.basicConfig(\n",
    "#         filename=log_filename,\n",
    "#         level=logging.INFO,\n",
    "#         format='%(asctime)s - %(message)s',\n",
    "#         filemode='w'  # Overwrite the log file for each run\n",
    "#     )\n",
    "#\n",
    "#     logging.info(f\"Starting evaluation for model: {model_id}\")\n",
    "#     test_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     ).to(\"cuda\")\n",
    "#\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_id, device=\"cuda\")\n",
    "#     pipe = pipeline(\"text-generation\", model=test_model, tokenizer=tokenizer, device=\"cuda\")\n",
    "#     all_metrics = []\n",
    "#\n",
    "#     for example in tqdm(test_set):\n",
    "#         prompt = example['prompt']\n",
    "#         # print(f\"Prompt:\\n{prompt}\")\n",
    "#         ground_truth = [item.strip() for item in example['completion'].split(\",\")]\n",
    "#\n",
    "#         # Generate recommendations\n",
    "#         message = [{\"role\": \"user\", \"content\": prompt}]\n",
    "#         outputs = pipe(message, max_new_tokens=256)\n",
    "#         required_output = outputs[0]['generated_text'][-1]['content']\n",
    "#         llm_generated_movies = extract_movie_titles(required_output)\n",
    "#         # print(f\"LLM Outputs:\\n{llm_generated_movies}\")\n",
    "#         # Log prompt and LLM output\n",
    "#         logging.info(\"Prompt: %s\", prompt)\n",
    "#         logging.info(\"LLM Output: %s\", required_output)\n",
    "#\n",
    "#         # Match generated movies to actual movie titles\n",
    "#         matched_titles_with_scores = match_titles_batch(llm_generated_movies, all_movies)\n",
    "# #         print(f'Matched titles: {matched_titles_with_scores}')\n",
    "#         recommended_movies = [title for title, score in matched_titles_with_scores if title is not None]\n",
    "# #         print(f'Recommended movies: {recommended_movies}')\n",
    "# #         print(f'Ground truth: {ground_truth}')\n",
    "#         # Log matched titles and recommended movies\n",
    "#         logging.info(\"Matched Titles: %s\", matched_titles_with_scores)\n",
    "#         logging.info(\"Recommended Movies: %s\", recommended_movies)\n",
    "#         logging.info(\"Ground Truth: %s\", ground_truth)\n",
    "#         # Evaluate recommendations\n",
    "#         metrics = evaluate_recommendations(ground_truth, recommended_movies, k)\n",
    "#         all_metrics.append(metrics)\n",
    "#\n",
    "#     # Calculate average metrics\n",
    "#     avg_metrics = {metric: sum(m[metric] for m in all_metrics) / len(all_metrics) for metric in all_metrics[0]}\n",
    "#     return avg_metrics\n"
   ],
   "id": "8d0c7bddf6cb8efa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T19:25:32.053368Z",
     "start_time": "2024-12-17T19:25:32.047506Z"
    }
   },
   "cell_type": "code",
   "source": "additional_instruction = \" Do not repeat the movies given in the prompt. Recommend movies based on the preference given. \"",
   "id": "50638dc674c7c7ae",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T19:07:34.136792Z",
     "start_time": "2024-12-17T19:07:34.131853Z"
    }
   },
   "cell_type": "code",
   "source": "from peft import PeftModel",
   "id": "132ac352e83cc295",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T18:49:58.629015Z",
     "start_time": "2024-12-17T18:49:58.607919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model(model_id, k=5):\n",
    "    # Configure structured logging\n",
    "    log_filename = f\"{model_id.replace('/','_')}_evaluation.jsonl\"  # Use a JSONL file for structured logging\n",
    "    logger = logging.getLogger(model_id)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    handler = logging.FileHandler(log_filename, mode='w')\n",
    "    handler.setFormatter(logging.Formatter('%(message)s'))\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    def log_step(data):\n",
    "        \"\"\"Log each step as a structured JSON object.\"\"\"\n",
    "        logger.info(json.dumps(data))\n",
    "\n",
    "    log_step({\"event\": \"evaluation_start\", \"model_id\": model_id})\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    test_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "    ).to(\"cuda\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, device=\"cuda\")\n",
    "    pipe = pipeline(\"text-generation\", model=test_model, tokenizer=tokenizer, device=\"cuda\")\n",
    "\n",
    "    all_metrics = []\n",
    "\n",
    "    for example in tqdm(test_set):\n",
    "        prompt = example['prompt'] + additional_instruction\n",
    "        ground_truth = [item.strip() for item in example['completion'].split(\",\")]\n",
    "\n",
    "        # Generate recommendations\n",
    "        message = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        outputs = pipe(message, max_new_tokens=256)\n",
    "        response = outputs[0]['generated_text'][-1]['content']\n",
    "        llm_generated_movies = extract_movie_titles(response)\n",
    "\n",
    "        # Match generated movies to actual movie titles\n",
    "        matched_titles_with_scores = match_titles_batch(llm_generated_movies, all_movies)\n",
    "        recommended_movies = [title for title, score in matched_titles_with_scores if title is not None]\n",
    "\n",
    "        # Evaluate recommendations\n",
    "        metrics = evaluate_recommendations(ground_truth, recommended_movies, k)\n",
    "        all_metrics.append(metrics)\n",
    "\n",
    "        # Log details for this example\n",
    "        log_step({\n",
    "            \"prompt\": prompt,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"llm_response\": response,\n",
    "            \"llm_output\": llm_generated_movies,\n",
    "            \"matched_titles_with_scores\": matched_titles_with_scores,\n",
    "            \"recommended_movies\": recommended_movies,\n",
    "            \"metrics\": metrics,\n",
    "        })\n",
    "\n",
    "    # Calculate and log average metrics\n",
    "    avg_metrics = {metric: sum(m[metric] for m in all_metrics) / len(all_metrics) for metric in all_metrics[0]}\n",
    "    log_step({\"event\": \"evaluation_end\", \"model_id\": model_id, \"average_metrics\": avg_metrics})\n",
    "\n",
    "    return avg_metrics"
   ],
   "id": "f6dc20ef9be35681",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T18:09:22.075778Z",
     "start_time": "2024-12-15T17:41:24.391760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pre_sft_model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "pre_sft_metrics = evaluate_model(pre_sft_model_id, k=5)"
   ],
   "id": "10c5f283a3f1dd25",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/864 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  0%|          | 1/864 [00:01<20:54,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  0%|          | 2/864 [00:02<17:44,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  0%|          | 3/864 [00:04<20:41,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  0%|          | 4/864 [00:05<19:58,  1.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  1%|          | 5/864 [00:06<19:34,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  1%|          | 6/864 [00:08<18:31,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  1%|          | 7/864 [00:18<1:02:07,  4.35s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  1%|          | 8/864 [00:19<46:28,  3.26s/it]  Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  1%|          | 9/864 [00:20<36:30,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  1%|          | 10/864 [00:22<33:26,  2.35s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  1%|▏         | 11/864 [00:23<27:05,  1.91s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  1%|▏         | 12/864 [00:34<1:08:42,  4.84s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  2%|▏         | 13/864 [00:36<55:03,  3.88s/it]  Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  2%|▏         | 14/864 [00:38<45:17,  3.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  2%|▏         | 15/864 [00:39<37:16,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  2%|▏         | 16/864 [00:41<32:22,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  2%|▏         | 17/864 [00:42<30:38,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  2%|▏         | 18/864 [00:44<27:49,  1.97s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  2%|▏         | 19/864 [00:45<25:19,  1.80s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  2%|▏         | 20/864 [00:46<22:07,  1.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  2%|▏         | 21/864 [00:48<20:35,  1.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  3%|▎         | 22/864 [00:59<1:02:23,  4.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  3%|▎         | 23/864 [01:00<48:04,  3.43s/it]  Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  3%|▎         | 24/864 [01:01<38:53,  2.78s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  3%|▎         | 25/864 [01:03<32:16,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  3%|▎         | 26/864 [01:04<27:30,  1.97s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  3%|▎         | 27/864 [01:05<24:33,  1.76s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  3%|▎         | 28/864 [01:06<23:18,  1.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  3%|▎         | 29/864 [01:18<1:02:45,  4.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  3%|▎         | 30/864 [01:19<48:37,  3.50s/it]  Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  4%|▎         | 31/864 [01:20<38:57,  2.81s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  4%|▎         | 32/864 [01:31<1:12:30,  5.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  4%|▍         | 33/864 [01:32<55:23,  4.00s/it]  Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  4%|▍         | 34/864 [01:43<1:26:37,  6.26s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  4%|▍         | 35/864 [01:45<1:05:34,  4.75s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  4%|▍         | 36/864 [01:46<50:50,  3.68s/it]  Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  4%|▍         | 37/864 [01:47<41:54,  3.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  4%|▍         | 38/864 [01:49<34:04,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  5%|▍         | 39/864 [01:50<30:17,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  5%|▍         | 40/864 [01:51<26:35,  1.94s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  5%|▍         | 41/864 [02:03<1:05:02,  4.74s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  5%|▍         | 42/864 [02:04<49:43,  3.63s/it]  Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  5%|▍         | 43/864 [02:09<57:42,  4.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  5%|▌         | 44/864 [02:11<48:29,  3.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  5%|▌         | 45/864 [02:12<38:09,  2.79s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  5%|▌         | 46/864 [02:15<38:08,  2.80s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  5%|▌         | 47/864 [02:16<31:06,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  6%|▌         | 48/864 [02:17<26:13,  1.93s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  6%|▌         | 49/864 [02:18<22:47,  1.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  6%|▌         | 50/864 [02:20<20:28,  1.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  6%|▌         | 51/864 [02:21<19:27,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  6%|▌         | 52/864 [02:22<17:58,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  6%|▌         | 53/864 [02:23<17:37,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  6%|▋         | 54/864 [02:24<17:09,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  6%|▋         | 55/864 [02:26<17:28,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  6%|▋         | 56/864 [02:27<17:12,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  7%|▋         | 57/864 [02:28<16:52,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  7%|▋         | 58/864 [02:29<16:02,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  7%|▋         | 59/864 [02:31<16:28,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  7%|▋         | 60/864 [02:32<15:58,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  7%|▋         | 61/864 [02:33<16:38,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  7%|▋         | 62/864 [02:34<16:36,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  7%|▋         | 63/864 [02:35<16:38,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  7%|▋         | 64/864 [02:37<15:49,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  8%|▊         | 65/864 [02:38<17:14,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  8%|▊         | 66/864 [02:39<17:02,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  8%|▊         | 67/864 [02:41<16:49,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  8%|▊         | 68/864 [02:42<15:44,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  8%|▊         | 69/864 [02:43<16:22,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  8%|▊         | 70/864 [02:44<17:17,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  8%|▊         | 71/864 [02:46<17:25,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  8%|▊         | 72/864 [02:47<17:00,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  8%|▊         | 73/864 [02:48<17:04,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  9%|▊         | 74/864 [02:49<16:35,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  9%|▊         | 75/864 [02:51<15:48,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  9%|▉         | 76/864 [02:52<15:54,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  9%|▉         | 77/864 [02:53<15:32,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  9%|▉         | 78/864 [02:54<14:54,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  9%|▉         | 79/864 [02:55<15:18,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  9%|▉         | 80/864 [02:56<15:02,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  9%|▉         | 81/864 [02:58<15:36,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  9%|▉         | 82/864 [02:59<14:53,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 10%|▉         | 83/864 [03:00<14:50,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 10%|▉         | 84/864 [03:01<16:18,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 10%|▉         | 85/864 [03:02<14:54,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 10%|▉         | 86/864 [03:05<19:56,  1.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 10%|█         | 87/864 [03:06<18:52,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 10%|█         | 88/864 [03:07<18:08,  1.40s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 10%|█         | 89/864 [03:08<17:45,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 10%|█         | 90/864 [03:10<17:12,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 11%|█         | 91/864 [03:11<17:08,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 11%|█         | 92/864 [03:12<16:39,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 11%|█         | 93/864 [03:13<16:28,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 11%|█         | 94/864 [03:24<52:36,  4.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 11%|█         | 95/864 [03:25<41:59,  3.28s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 11%|█         | 96/864 [03:27<33:21,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 11%|█         | 97/864 [03:28<28:04,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 11%|█▏        | 98/864 [03:39<1:02:17,  4.88s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 11%|█▏        | 99/864 [03:40<48:31,  3.81s/it]  Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 12%|█▏        | 100/864 [03:42<39:06,  3.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 12%|█▏        | 101/864 [03:43<32:32,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 12%|█▏        | 102/864 [03:44<27:08,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 12%|█▏        | 103/864 [03:46<24:33,  1.94s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 12%|█▏        | 104/864 [03:47<22:00,  1.74s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 12%|█▏        | 105/864 [03:48<20:09,  1.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 12%|█▏        | 106/864 [03:49<19:03,  1.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 12%|█▏        | 107/864 [03:50<17:10,  1.36s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 12%|█▎        | 108/864 [03:53<22:31,  1.79s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 13%|█▎        | 109/864 [03:54<20:19,  1.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 13%|█▎        | 110/864 [04:05<54:03,  4.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 13%|█▎        | 111/864 [04:06<41:18,  3.29s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 13%|█▎        | 112/864 [04:07<33:02,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 13%|█▎        | 113/864 [04:08<27:30,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 13%|█▎        | 114/864 [04:09<23:45,  1.90s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 13%|█▎        | 115/864 [04:11<21:25,  1.72s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 13%|█▎        | 116/864 [04:12<19:44,  1.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 14%|█▎        | 117/864 [04:13<17:41,  1.42s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 14%|█▎        | 118/864 [04:14<16:44,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 14%|█▍        | 119/864 [04:15<15:18,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 14%|█▍        | 120/864 [04:20<27:13,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 14%|█▍        | 121/864 [04:21<23:35,  1.90s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 14%|█▍        | 122/864 [04:22<20:18,  1.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 14%|█▍        | 123/864 [04:23<19:06,  1.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 14%|█▍        | 124/864 [04:24<17:55,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 14%|█▍        | 125/864 [04:26<17:21,  1.41s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 15%|█▍        | 126/864 [04:27<16:45,  1.36s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 15%|█▍        | 127/864 [04:28<16:12,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 15%|█▍        | 128/864 [04:29<15:59,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 15%|█▍        | 129/864 [04:31<15:13,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 15%|█▌        | 130/864 [04:32<16:15,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 15%|█▌        | 131/864 [04:34<16:52,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 15%|█▌        | 132/864 [04:35<17:28,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 15%|█▌        | 133/864 [04:36<15:55,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 16%|█▌        | 134/864 [04:37<15:29,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 16%|█▌        | 135/864 [04:39<15:12,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 16%|█▌        | 136/864 [04:40<14:22,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 16%|█▌        | 137/864 [04:41<14:09,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 16%|█▌        | 138/864 [04:51<49:01,  4.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 16%|█▌        | 139/864 [04:59<59:51,  4.95s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 16%|█▌        | 140/864 [05:00<45:28,  3.77s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 16%|█▋        | 141/864 [05:01<35:55,  2.98s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 16%|█▋        | 142/864 [05:02<30:50,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 17%|█▋        | 143/864 [05:04<26:31,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 17%|█▋        | 144/864 [05:05<23:55,  1.99s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 17%|█▋        | 145/864 [05:07<22:42,  1.89s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 17%|█▋        | 146/864 [05:08<20:23,  1.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 17%|█▋        | 147/864 [05:09<18:04,  1.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 17%|█▋        | 148/864 [05:11<18:13,  1.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 17%|█▋        | 149/864 [05:12<17:04,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 17%|█▋        | 150/864 [05:13<16:03,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 17%|█▋        | 151/864 [05:14<15:53,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 18%|█▊        | 152/864 [05:16<18:26,  1.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 18%|█▊        | 153/864 [05:18<17:58,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 18%|█▊        | 154/864 [05:19<16:23,  1.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 18%|█▊        | 155/864 [05:20<15:17,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 18%|█▊        | 156/864 [05:21<15:24,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 18%|█▊        | 157/864 [05:32<47:12,  4.01s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 18%|█▊        | 158/864 [05:33<36:56,  3.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 18%|█▊        | 159/864 [05:34<30:09,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 19%|█▊        | 160/864 [05:35<24:34,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 19%|█▊        | 161/864 [05:36<21:19,  1.82s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 19%|█▉        | 162/864 [05:38<19:59,  1.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 19%|█▉        | 163/864 [05:39<18:27,  1.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 19%|█▉        | 164/864 [05:40<16:35,  1.42s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 19%|█▉        | 165/864 [05:41<15:54,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 19%|█▉        | 166/864 [05:43<15:47,  1.36s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 19%|█▉        | 167/864 [05:43<14:25,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 19%|█▉        | 168/864 [05:46<19:30,  1.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 20%|█▉        | 169/864 [05:47<17:05,  1.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 20%|█▉        | 170/864 [05:49<16:46,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 20%|█▉        | 171/864 [05:59<48:53,  4.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 20%|█▉        | 172/864 [06:01<38:33,  3.34s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 20%|██        | 173/864 [06:02<30:55,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 20%|██        | 174/864 [06:03<25:29,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 20%|██        | 175/864 [06:04<21:45,  1.89s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 20%|██        | 176/864 [06:16<55:40,  4.86s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 20%|██        | 177/864 [06:17<42:25,  3.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 21%|██        | 178/864 [06:18<35:18,  3.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 21%|██        | 179/864 [06:20<29:31,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 21%|██        | 180/864 [06:21<23:56,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 21%|██        | 181/864 [06:22<20:14,  1.78s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 21%|██        | 182/864 [06:23<18:11,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 21%|██        | 183/864 [06:24<17:08,  1.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 21%|██▏       | 184/864 [06:29<28:48,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 21%|██▏       | 185/864 [06:40<56:09,  4.96s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 22%|██▏       | 186/864 [06:42<44:54,  3.97s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 22%|██▏       | 187/864 [06:43<35:49,  3.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 22%|██▏       | 188/864 [06:44<29:08,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 22%|██▏       | 189/864 [06:56<1:00:54,  5.41s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 22%|██▏       | 190/864 [06:57<46:47,  4.17s/it]  Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 22%|██▏       | 191/864 [06:59<36:36,  3.26s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 22%|██▏       | 192/864 [07:00<31:24,  2.80s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 22%|██▏       | 193/864 [07:01<26:01,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 22%|██▏       | 194/864 [07:03<22:35,  2.02s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 23%|██▎       | 195/864 [07:04<20:18,  1.82s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 23%|██▎       | 196/864 [07:05<17:46,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 23%|██▎       | 197/864 [07:06<16:19,  1.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 23%|██▎       | 198/864 [07:08<16:06,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 23%|██▎       | 199/864 [07:09<14:44,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 23%|██▎       | 200/864 [07:10<14:09,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 23%|██▎       | 201/864 [07:11<14:22,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 23%|██▎       | 202/864 [07:13<14:22,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 23%|██▎       | 203/864 [07:14<14:03,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 24%|██▎       | 204/864 [07:15<12:53,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 24%|██▎       | 205/864 [07:16<13:59,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 24%|██▍       | 206/864 [07:18<14:12,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 24%|██▍       | 207/864 [07:19<13:38,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 24%|██▍       | 208/864 [07:20<13:19,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 24%|██▍       | 209/864 [07:21<13:33,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 24%|██▍       | 210/864 [07:22<12:52,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 24%|██▍       | 211/864 [07:24<13:14,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 25%|██▍       | 212/864 [07:25<14:02,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 25%|██▍       | 213/864 [07:26<13:22,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 25%|██▍       | 214/864 [07:27<13:25,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 25%|██▍       | 215/864 [07:29<13:27,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 25%|██▌       | 216/864 [07:30<13:17,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 25%|██▌       | 217/864 [07:31<12:46,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 25%|██▌       | 218/864 [07:32<13:13,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 25%|██▌       | 219/864 [07:34<14:47,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 25%|██▌       | 220/864 [07:35<13:53,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 26%|██▌       | 221/864 [07:36<13:28,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 26%|██▌       | 222/864 [07:37<13:17,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 26%|██▌       | 223/864 [07:39<13:43,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 26%|██▌       | 224/864 [07:40<12:48,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 26%|██▌       | 225/864 [07:41<12:31,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 26%|██▌       | 226/864 [07:42<13:24,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 26%|██▋       | 227/864 [07:44<14:13,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 26%|██▋       | 228/864 [07:45<14:32,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 27%|██▋       | 229/864 [07:56<45:28,  4.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 27%|██▋       | 230/864 [07:58<35:34,  3.37s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 27%|██▋       | 231/864 [07:59<28:43,  2.72s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 27%|██▋       | 232/864 [08:06<41:54,  3.98s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 27%|██▋       | 233/864 [08:07<32:29,  3.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 27%|██▋       | 234/864 [08:09<28:10,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 27%|██▋       | 235/864 [08:10<23:20,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 27%|██▋       | 236/864 [08:11<19:45,  1.89s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 27%|██▋       | 237/864 [08:12<17:14,  1.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 28%|██▊       | 238/864 [08:13<15:23,  1.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 28%|██▊       | 239/864 [08:14<14:15,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 28%|██▊       | 240/864 [08:15<13:28,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 28%|██▊       | 241/864 [08:16<12:39,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 28%|██▊       | 242/864 [08:18<14:13,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 28%|██▊       | 243/864 [08:19<13:56,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 28%|██▊       | 244/864 [08:21<13:30,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 28%|██▊       | 245/864 [08:22<12:57,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 28%|██▊       | 246/864 [08:23<12:43,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 29%|██▊       | 247/864 [08:24<11:59,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 29%|██▊       | 248/864 [08:25<11:53,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 29%|██▉       | 249/864 [08:34<36:19,  3.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 29%|██▉       | 250/864 [08:45<59:33,  5.82s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 29%|██▉       | 251/864 [08:46<44:39,  4.37s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 29%|██▉       | 252/864 [08:48<35:11,  3.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 29%|██▉       | 253/864 [08:49<27:45,  2.73s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 29%|██▉       | 254/864 [08:50<22:38,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 30%|██▉       | 255/864 [09:01<49:35,  4.89s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 30%|██▉       | 256/864 [09:02<37:51,  3.74s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 30%|██▉       | 257/864 [09:03<30:29,  3.01s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 30%|██▉       | 258/864 [09:04<24:45,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 30%|██▉       | 259/864 [09:06<21:34,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 30%|███       | 260/864 [09:07<18:17,  1.82s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 30%|███       | 261/864 [09:08<17:57,  1.79s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 30%|███       | 262/864 [09:10<16:00,  1.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 30%|███       | 263/864 [09:11<14:11,  1.42s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 31%|███       | 264/864 [09:12<14:20,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 31%|███       | 265/864 [09:14<16:48,  1.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 31%|███       | 266/864 [09:16<15:28,  1.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 31%|███       | 267/864 [09:17<14:26,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 31%|███       | 268/864 [09:18<13:38,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 31%|███       | 269/864 [09:19<13:00,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 31%|███▏      | 270/864 [09:20<12:38,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 31%|███▏      | 271/864 [09:22<12:24,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 31%|███▏      | 272/864 [09:23<12:39,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 32%|███▏      | 273/864 [09:24<13:05,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 32%|███▏      | 274/864 [09:26<12:42,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 32%|███▏      | 275/864 [09:27<12:16,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 32%|███▏      | 276/864 [09:28<12:39,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 32%|███▏      | 277/864 [09:29<11:55,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 32%|███▏      | 278/864 [09:30<11:42,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 32%|███▏      | 279/864 [09:31<11:33,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 32%|███▏      | 280/864 [09:33<11:39,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 33%|███▎      | 281/864 [09:34<11:26,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 33%|███▎      | 282/864 [09:35<11:56,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 33%|███▎      | 283/864 [09:36<11:53,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 33%|███▎      | 284/864 [09:37<10:49,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 33%|███▎      | 285/864 [09:39<11:23,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 33%|███▎      | 286/864 [09:40<11:51,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 33%|███▎      | 287/864 [09:42<14:15,  1.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 33%|███▎      | 288/864 [09:43<12:54,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 33%|███▎      | 289/864 [09:44<12:51,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 34%|███▎      | 290/864 [09:45<11:58,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 34%|███▎      | 291/864 [09:47<14:13,  1.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 34%|███▍      | 292/864 [09:49<13:56,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 34%|███▍      | 293/864 [09:50<13:52,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 34%|███▍      | 294/864 [09:51<12:22,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 34%|███▍      | 295/864 [09:52<12:20,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 34%|███▍      | 296/864 [09:53<11:22,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 34%|███▍      | 297/864 [09:55<11:21,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 34%|███▍      | 298/864 [10:05<37:05,  3.93s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 35%|███▍      | 299/864 [10:06<29:46,  3.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 35%|███▍      | 300/864 [10:08<24:08,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 35%|███▍      | 301/864 [10:09<20:24,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 35%|███▍      | 302/864 [10:10<17:28,  1.87s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 35%|███▌      | 303/864 [10:11<15:45,  1.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 35%|███▌      | 304/864 [10:12<14:26,  1.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 35%|███▌      | 305/864 [10:13<13:06,  1.41s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 35%|███▌      | 306/864 [10:25<40:26,  4.35s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 36%|███▌      | 307/864 [10:26<31:51,  3.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 36%|███▌      | 308/864 [10:29<30:44,  3.32s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 36%|███▌      | 309/864 [10:30<24:09,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 36%|███▌      | 310/864 [10:31<20:18,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 36%|███▌      | 311/864 [10:32<17:01,  1.85s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 36%|███▌      | 312/864 [10:34<15:25,  1.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 36%|███▌      | 313/864 [10:35<13:44,  1.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 36%|███▋      | 314/864 [10:36<12:55,  1.41s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 36%|███▋      | 315/864 [10:37<11:55,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 37%|███▋      | 316/864 [10:38<11:51,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 37%|███▋      | 317/864 [10:40<12:56,  1.42s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 37%|███▋      | 318/864 [10:41<12:16,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 37%|███▋      | 319/864 [10:42<11:50,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 37%|███▋      | 320/864 [10:44<12:06,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 37%|███▋      | 321/864 [10:45<11:45,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 37%|███▋      | 322/864 [10:46<11:41,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 37%|███▋      | 323/864 [10:48<12:22,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 38%|███▊      | 324/864 [10:49<12:05,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 38%|███▊      | 325/864 [10:50<11:46,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 38%|███▊      | 326/864 [10:51<11:29,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 38%|███▊      | 327/864 [10:53<11:48,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 38%|███▊      | 328/864 [10:54<11:34,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 38%|███▊      | 329/864 [10:55<11:07,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 38%|███▊      | 330/864 [10:56<11:10,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 38%|███▊      | 331/864 [10:58<11:22,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 38%|███▊      | 332/864 [11:03<21:50,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 39%|███▊      | 333/864 [11:05<19:45,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 39%|███▊      | 334/864 [11:06<17:41,  2.00s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 39%|███▉      | 335/864 [11:07<15:30,  1.76s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 39%|███▉      | 336/864 [11:08<13:21,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 39%|███▉      | 337/864 [11:10<13:01,  1.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 39%|███▉      | 338/864 [11:21<38:40,  4.41s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 39%|███▉      | 339/864 [11:22<30:29,  3.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 39%|███▉      | 340/864 [11:24<24:51,  2.85s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 39%|███▉      | 341/864 [11:25<21:08,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 40%|███▉      | 342/864 [11:26<17:33,  2.02s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 40%|███▉      | 343/864 [11:28<15:47,  1.82s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 40%|███▉      | 344/864 [11:29<14:20,  1.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 40%|███▉      | 345/864 [11:30<13:16,  1.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 40%|████      | 346/864 [11:31<12:30,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 40%|████      | 347/864 [11:35<17:59,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 40%|████      | 348/864 [11:36<16:06,  1.87s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 40%|████      | 349/864 [11:37<13:47,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 41%|████      | 350/864 [11:39<13:42,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 41%|████      | 351/864 [11:40<13:08,  1.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 41%|████      | 352/864 [11:41<11:58,  1.40s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 41%|████      | 353/864 [11:43<11:45,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 41%|████      | 354/864 [11:44<11:06,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 41%|████      | 355/864 [11:45<11:11,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 41%|████      | 356/864 [11:46<10:24,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 41%|████▏     | 357/864 [11:47<09:58,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 41%|████▏     | 358/864 [11:50<13:00,  1.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 42%|████▏     | 359/864 [11:51<11:33,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 42%|████▏     | 360/864 [11:51<10:18,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 42%|████▏     | 361/864 [11:53<10:18,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 42%|████▏     | 362/864 [11:54<10:25,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 42%|████▏     | 363/864 [11:55<10:14,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 42%|████▏     | 364/864 [11:57<10:30,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 42%|████▏     | 365/864 [11:58<10:27,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 42%|████▏     | 366/864 [11:59<10:12,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 42%|████▏     | 367/864 [12:01<11:44,  1.42s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 43%|████▎     | 368/864 [12:02<10:59,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 43%|████▎     | 369/864 [12:03<10:26,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 43%|████▎     | 370/864 [12:04<09:50,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 43%|████▎     | 371/864 [12:05<09:28,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 43%|████▎     | 372/864 [12:07<11:28,  1.40s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 43%|████▎     | 373/864 [12:08<10:59,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 43%|████▎     | 374/864 [12:10<11:15,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 43%|████▎     | 375/864 [12:11<10:48,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 44%|████▎     | 376/864 [12:12<10:38,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 44%|████▎     | 377/864 [12:14<11:04,  1.36s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 44%|████▍     | 378/864 [12:15<11:11,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 44%|████▍     | 379/864 [12:16<10:34,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 44%|████▍     | 380/864 [12:17<10:17,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 44%|████▍     | 381/864 [12:19<09:48,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 44%|████▍     | 382/864 [12:20<09:22,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 44%|████▍     | 383/864 [12:21<09:28,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 44%|████▍     | 384/864 [12:23<10:55,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 45%|████▍     | 385/864 [12:24<09:57,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 45%|████▍     | 386/864 [12:25<10:04,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 45%|████▍     | 387/864 [12:26<09:58,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 45%|████▍     | 388/864 [12:27<09:49,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 45%|████▌     | 389/864 [12:29<09:52,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 45%|████▌     | 390/864 [12:30<09:52,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 45%|████▌     | 391/864 [12:31<09:47,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 45%|████▌     | 392/864 [12:32<09:30,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 45%|████▌     | 393/864 [12:33<09:19,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 46%|████▌     | 394/864 [12:34<09:05,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 46%|████▌     | 395/864 [12:36<09:04,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 46%|████▌     | 396/864 [12:37<08:53,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 46%|████▌     | 397/864 [12:38<08:53,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 46%|████▌     | 398/864 [12:39<09:01,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 46%|████▌     | 399/864 [12:41<09:51,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 46%|████▋     | 400/864 [12:42<09:57,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 46%|████▋     | 401/864 [12:43<09:31,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 47%|████▋     | 402/864 [12:45<10:10,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 47%|████▋     | 403/864 [12:46<09:31,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 47%|████▋     | 404/864 [12:47<09:35,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 47%|████▋     | 405/864 [12:48<08:54,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 47%|████▋     | 406/864 [12:49<09:30,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 47%|████▋     | 407/864 [12:50<09:22,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 47%|████▋     | 408/864 [12:52<09:16,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 47%|████▋     | 409/864 [12:53<08:58,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 47%|████▋     | 410/864 [12:54<08:37,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 48%|████▊     | 411/864 [12:55<08:18,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 48%|████▊     | 412/864 [12:56<08:48,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 48%|████▊     | 413/864 [12:57<08:29,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 48%|████▊     | 414/864 [12:58<08:10,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 48%|████▊     | 415/864 [12:59<08:39,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 48%|████▊     | 416/864 [13:01<08:48,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 48%|████▊     | 417/864 [13:02<09:01,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 48%|████▊     | 418/864 [13:03<08:52,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 48%|████▊     | 419/864 [13:05<09:33,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 49%|████▊     | 420/864 [13:16<32:30,  4.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 49%|████▊     | 421/864 [13:17<25:04,  3.40s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 49%|████▉     | 422/864 [13:18<19:39,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 49%|████▉     | 423/864 [13:20<16:51,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 49%|████▉     | 424/864 [13:21<14:17,  1.95s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 49%|████▉     | 425/864 [13:22<12:30,  1.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 49%|████▉     | 426/864 [13:23<10:50,  1.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 49%|████▉     | 427/864 [13:26<13:25,  1.84s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 50%|████▉     | 428/864 [13:27<12:36,  1.73s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 50%|████▉     | 429/864 [13:30<15:04,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 50%|████▉     | 430/864 [13:31<12:56,  1.79s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 50%|████▉     | 431/864 [13:32<11:34,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 50%|█████     | 432/864 [13:33<10:22,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 50%|█████     | 433/864 [13:37<15:35,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 50%|█████     | 434/864 [13:38<13:23,  1.87s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 50%|█████     | 435/864 [13:40<11:51,  1.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 50%|█████     | 436/864 [13:41<10:28,  1.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 51%|█████     | 437/864 [13:42<09:58,  1.40s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 51%|█████     | 438/864 [13:44<10:27,  1.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 51%|█████     | 439/864 [13:45<09:40,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 51%|█████     | 440/864 [13:46<08:43,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 51%|█████     | 441/864 [13:47<08:36,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 51%|█████     | 442/864 [13:48<08:34,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 51%|█████▏    | 443/864 [13:49<08:25,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 51%|█████▏    | 444/864 [13:50<07:57,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 52%|█████▏    | 445/864 [13:51<08:27,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 52%|█████▏    | 446/864 [13:53<08:36,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 52%|█████▏    | 447/864 [13:54<08:47,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 52%|█████▏    | 448/864 [14:05<28:34,  4.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 52%|█████▏    | 449/864 [14:07<24:41,  3.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 52%|█████▏    | 450/864 [14:08<19:30,  2.83s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 52%|█████▏    | 451/864 [14:10<17:00,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 52%|█████▏    | 452/864 [14:11<14:52,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 52%|█████▏    | 453/864 [14:13<13:36,  1.99s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 53%|█████▎    | 454/864 [14:14<11:32,  1.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 53%|█████▎    | 455/864 [14:15<10:59,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 53%|█████▎    | 456/864 [14:17<10:06,  1.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 53%|█████▎    | 457/864 [14:18<10:03,  1.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 53%|█████▎    | 458/864 [14:19<09:22,  1.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 53%|█████▎    | 459/864 [14:21<09:33,  1.42s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 53%|█████▎    | 460/864 [14:22<08:38,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 53%|█████▎    | 461/864 [14:23<09:37,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 53%|█████▎    | 462/864 [14:25<09:00,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 54%|█████▎    | 463/864 [14:36<28:15,  4.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 54%|█████▎    | 464/864 [14:38<23:51,  3.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 54%|█████▍    | 465/864 [14:38<18:27,  2.78s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 54%|█████▍    | 466/864 [14:50<35:10,  5.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 54%|█████▍    | 467/864 [14:51<26:25,  3.99s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 54%|█████▍    | 468/864 [14:57<30:15,  4.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 54%|█████▍    | 469/864 [14:58<23:06,  3.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 54%|█████▍    | 470/864 [14:59<19:12,  2.92s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 55%|█████▍    | 471/864 [15:00<15:25,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 55%|█████▍    | 472/864 [15:01<12:33,  1.92s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 55%|█████▍    | 473/864 [15:02<11:26,  1.76s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 55%|█████▍    | 474/864 [15:04<10:10,  1.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 55%|█████▍    | 475/864 [15:05<08:56,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 55%|█████▌    | 476/864 [15:05<08:04,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 55%|█████▌    | 477/864 [15:07<07:57,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 55%|█████▌    | 478/864 [15:08<07:38,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 55%|█████▌    | 479/864 [15:09<07:21,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 56%|█████▌    | 480/864 [15:10<07:48,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 56%|█████▌    | 481/864 [15:12<08:28,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 56%|█████▌    | 482/864 [15:13<07:56,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 56%|█████▌    | 483/864 [15:24<27:11,  4.28s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 56%|█████▌    | 484/864 [15:25<21:18,  3.37s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 56%|█████▌    | 485/864 [15:26<16:51,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 56%|█████▋    | 486/864 [15:28<15:09,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 56%|█████▋    | 487/864 [15:30<13:04,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 56%|█████▋    | 488/864 [15:31<11:26,  1.83s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 57%|█████▋    | 489/864 [15:32<09:44,  1.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 57%|█████▋    | 490/864 [15:33<08:58,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 57%|█████▋    | 491/864 [15:34<08:18,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 57%|█████▋    | 492/864 [15:35<08:16,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 57%|█████▋    | 493/864 [15:37<08:09,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 57%|█████▋    | 494/864 [15:38<07:28,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 57%|█████▋    | 495/864 [15:39<07:38,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 57%|█████▋    | 496/864 [15:40<07:40,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 58%|█████▊    | 497/864 [15:41<07:13,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 58%|█████▊    | 498/864 [15:43<07:31,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 58%|█████▊    | 499/864 [15:44<07:28,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 58%|█████▊    | 500/864 [15:45<07:44,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 58%|█████▊    | 501/864 [15:46<07:20,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 58%|█████▊    | 502/864 [15:49<10:53,  1.80s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 58%|█████▊    | 503/864 [15:50<09:34,  1.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 58%|█████▊    | 504/864 [15:58<20:59,  3.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 58%|█████▊    | 505/864 [16:00<16:34,  2.77s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 59%|█████▊    | 506/864 [16:01<13:55,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 59%|█████▊    | 507/864 [16:02<12:04,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 59%|█████▉    | 508/864 [16:03<10:16,  1.73s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 59%|█████▉    | 509/864 [16:04<09:25,  1.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 59%|█████▉    | 510/864 [16:06<08:27,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 59%|█████▉    | 511/864 [16:07<08:15,  1.40s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 59%|█████▉    | 512/864 [16:08<08:00,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 59%|█████▉    | 513/864 [16:10<08:02,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 59%|█████▉    | 514/864 [16:11<07:32,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 60%|█████▉    | 515/864 [16:12<06:59,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 60%|█████▉    | 516/864 [16:13<07:29,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 60%|█████▉    | 517/864 [16:14<07:19,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 60%|█████▉    | 518/864 [16:15<07:03,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 60%|██████    | 519/864 [16:17<07:18,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 60%|██████    | 520/864 [16:26<20:25,  3.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 60%|██████    | 521/864 [16:27<16:04,  2.81s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 60%|██████    | 522/864 [16:28<13:11,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 61%|██████    | 523/864 [16:29<11:23,  2.00s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 61%|██████    | 524/864 [16:41<27:22,  4.83s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 61%|██████    | 525/864 [16:42<21:12,  3.75s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 61%|██████    | 526/864 [16:43<16:44,  2.97s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 61%|██████    | 527/864 [16:44<13:51,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 61%|██████    | 528/864 [16:46<11:49,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 61%|██████    | 529/864 [16:47<10:22,  1.86s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 61%|██████▏   | 530/864 [16:48<09:08,  1.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 61%|██████▏   | 531/864 [16:49<08:30,  1.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 62%|██████▏   | 532/864 [16:50<07:46,  1.40s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 62%|██████▏   | 533/864 [16:52<07:35,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 62%|██████▏   | 534/864 [16:53<07:00,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 62%|██████▏   | 535/864 [16:54<06:45,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 62%|██████▏   | 536/864 [17:04<20:28,  3.74s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 62%|██████▏   | 537/864 [17:05<16:04,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 62%|██████▏   | 538/864 [17:06<13:57,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 62%|██████▏   | 539/864 [17:08<13:00,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 62%|██████▎   | 540/864 [17:09<10:46,  1.99s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 63%|██████▎   | 541/864 [17:11<09:43,  1.81s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 63%|██████▎   | 542/864 [17:13<11:13,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 63%|██████▎   | 543/864 [17:15<10:32,  1.97s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 63%|██████▎   | 544/864 [17:16<09:15,  1.74s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 63%|██████▎   | 545/864 [17:18<08:32,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 63%|██████▎   | 546/864 [17:19<08:08,  1.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 63%|██████▎   | 547/864 [17:20<07:22,  1.40s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 63%|██████▎   | 548/864 [17:21<07:18,  1.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 64%|██████▎   | 549/864 [17:24<08:46,  1.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 64%|██████▎   | 550/864 [17:25<08:32,  1.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 64%|██████▍   | 551/864 [17:27<08:01,  1.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 64%|██████▍   | 552/864 [17:28<07:55,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 64%|██████▍   | 553/864 [17:30<07:58,  1.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 64%|██████▍   | 554/864 [17:32<08:45,  1.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 64%|██████▍   | 555/864 [17:34<09:40,  1.88s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 64%|██████▍   | 556/864 [17:35<08:19,  1.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 64%|██████▍   | 557/864 [17:36<07:28,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 65%|██████▍   | 558/864 [17:37<07:04,  1.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 65%|██████▍   | 559/864 [17:39<07:05,  1.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 65%|██████▍   | 560/864 [17:40<06:41,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 65%|██████▍   | 561/864 [17:47<15:08,  3.00s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 65%|██████▌   | 562/864 [17:48<12:17,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 65%|██████▌   | 563/864 [17:49<10:13,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 65%|██████▌   | 564/864 [18:00<23:52,  4.77s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 65%|██████▌   | 565/864 [18:01<18:21,  3.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 66%|██████▌   | 566/864 [18:03<14:35,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 66%|██████▌   | 567/864 [18:04<12:03,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 66%|██████▌   | 568/864 [18:05<09:44,  1.98s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 66%|██████▌   | 569/864 [18:06<08:35,  1.75s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 66%|██████▌   | 570/864 [18:07<07:42,  1.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 66%|██████▌   | 571/864 [18:08<07:09,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 66%|██████▌   | 572/864 [18:10<07:22,  1.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 66%|██████▋   | 573/864 [18:11<06:31,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 66%|██████▋   | 574/864 [18:12<06:17,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 67%|██████▋   | 575/864 [18:14<06:21,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 67%|██████▋   | 576/864 [18:15<06:05,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 67%|██████▋   | 577/864 [18:16<05:47,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 67%|██████▋   | 578/864 [18:17<05:33,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 67%|██████▋   | 579/864 [18:18<05:26,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 67%|██████▋   | 580/864 [18:19<05:47,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 67%|██████▋   | 581/864 [18:20<05:41,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 67%|██████▋   | 582/864 [18:22<05:43,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 67%|██████▋   | 583/864 [18:23<06:21,  1.36s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 68%|██████▊   | 584/864 [18:35<20:23,  4.37s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 68%|██████▊   | 585/864 [18:36<15:42,  3.38s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 68%|██████▊   | 586/864 [18:37<12:35,  2.72s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 68%|██████▊   | 587/864 [18:38<10:28,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 68%|██████▊   | 588/864 [18:39<08:58,  1.95s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 68%|██████▊   | 589/864 [18:41<08:35,  1.87s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 68%|██████▊   | 590/864 [18:42<07:31,  1.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 68%|██████▊   | 591/864 [18:52<19:03,  4.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 69%|██████▊   | 592/864 [18:54<14:50,  3.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 69%|██████▊   | 593/864 [18:55<11:40,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 69%|██████▉   | 594/864 [18:56<09:54,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 69%|██████▉   | 595/864 [18:57<08:42,  1.94s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 69%|██████▉   | 596/864 [18:58<07:25,  1.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 69%|██████▉   | 597/864 [19:00<07:45,  1.74s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 69%|██████▉   | 598/864 [19:01<06:48,  1.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 69%|██████▉   | 599/864 [19:02<06:19,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 69%|██████▉   | 600/864 [19:13<19:01,  4.32s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 70%|██████▉   | 601/864 [19:14<14:38,  3.34s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 70%|██████▉   | 602/864 [19:16<11:44,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 70%|██████▉   | 603/864 [19:27<22:48,  5.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 70%|██████▉   | 604/864 [19:28<17:14,  3.98s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 70%|███████   | 605/864 [19:29<13:43,  3.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 70%|███████   | 606/864 [19:41<24:37,  5.72s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 70%|███████   | 607/864 [19:42<18:26,  4.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 70%|███████   | 608/864 [19:43<14:32,  3.41s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 70%|███████   | 609/864 [19:44<11:41,  2.75s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 71%|███████   | 610/864 [19:46<09:42,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 71%|███████   | 611/864 [19:47<08:11,  1.94s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 71%|███████   | 612/864 [19:48<07:06,  1.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 71%|███████   | 613/864 [19:59<18:44,  4.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 71%|███████   | 614/864 [20:00<14:53,  3.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 71%|███████   | 615/864 [20:02<12:04,  2.91s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 71%|███████▏  | 616/864 [20:13<22:45,  5.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 71%|███████▏  | 617/864 [20:15<18:18,  4.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 72%|███████▏  | 618/864 [20:16<13:55,  3.40s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 72%|███████▏  | 619/864 [20:17<11:01,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 72%|███████▏  | 620/864 [20:18<09:01,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 72%|███████▏  | 621/864 [20:19<07:35,  1.87s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 72%|███████▏  | 622/864 [20:21<06:53,  1.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 72%|███████▏  | 623/864 [20:22<06:10,  1.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 72%|███████▏  | 624/864 [20:23<06:12,  1.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 72%|███████▏  | 625/864 [20:25<06:14,  1.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 72%|███████▏  | 626/864 [20:26<05:57,  1.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 73%|███████▎  | 627/864 [20:27<05:26,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 73%|███████▎  | 628/864 [20:28<04:59,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 73%|███████▎  | 629/864 [20:30<04:47,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 73%|███████▎  | 630/864 [20:31<04:34,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 73%|███████▎  | 631/864 [20:32<04:53,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 73%|███████▎  | 632/864 [20:33<04:51,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 73%|███████▎  | 633/864 [20:34<04:34,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 73%|███████▎  | 634/864 [20:36<04:29,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 73%|███████▎  | 635/864 [20:37<05:02,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 74%|███████▎  | 636/864 [20:38<04:49,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 74%|███████▎  | 637/864 [20:39<04:41,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 74%|███████▍  | 638/864 [20:41<04:32,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 74%|███████▍  | 639/864 [20:42<04:27,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 74%|███████▍  | 640/864 [20:43<04:57,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 74%|███████▍  | 641/864 [20:45<04:46,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 74%|███████▍  | 642/864 [20:46<04:31,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 74%|███████▍  | 643/864 [20:47<04:18,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 75%|███████▍  | 644/864 [20:49<05:01,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 75%|███████▍  | 645/864 [20:50<04:49,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 75%|███████▍  | 646/864 [20:51<04:38,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 75%|███████▍  | 647/864 [20:52<04:26,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 75%|███████▌  | 648/864 [20:53<04:32,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 75%|███████▌  | 649/864 [20:55<04:31,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 75%|███████▌  | 650/864 [20:56<04:39,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 75%|███████▌  | 651/864 [20:57<04:27,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 75%|███████▌  | 652/864 [20:59<04:36,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 76%|███████▌  | 653/864 [21:00<04:17,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 76%|███████▌  | 654/864 [21:01<04:15,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 76%|███████▌  | 655/864 [21:02<04:16,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 76%|███████▌  | 656/864 [21:03<04:13,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 76%|███████▌  | 657/864 [21:05<04:18,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 76%|███████▌  | 658/864 [21:06<04:25,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 76%|███████▋  | 659/864 [21:08<05:12,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 76%|███████▋  | 660/864 [21:09<04:41,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 77%|███████▋  | 661/864 [21:10<04:34,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 77%|███████▋  | 662/864 [21:12<04:42,  1.40s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 77%|███████▋  | 663/864 [21:13<04:47,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 77%|███████▋  | 664/864 [21:15<04:38,  1.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 77%|███████▋  | 665/864 [21:17<05:05,  1.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 77%|███████▋  | 666/864 [21:18<05:09,  1.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 77%|███████▋  | 667/864 [21:19<04:44,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 77%|███████▋  | 668/864 [21:21<04:45,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 77%|███████▋  | 669/864 [21:22<04:33,  1.40s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 78%|███████▊  | 670/864 [21:24<04:55,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 78%|███████▊  | 671/864 [21:35<13:51,  4.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 78%|███████▊  | 672/864 [21:36<10:48,  3.38s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 78%|███████▊  | 673/864 [21:37<08:52,  2.79s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 78%|███████▊  | 674/864 [21:48<16:24,  5.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 78%|███████▊  | 675/864 [21:52<15:12,  4.83s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 78%|███████▊  | 676/864 [21:53<11:49,  3.78s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 78%|███████▊  | 677/864 [21:55<09:13,  2.96s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 78%|███████▊  | 678/864 [21:57<08:36,  2.78s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 79%|███████▊  | 679/864 [21:58<06:59,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 79%|███████▊  | 680/864 [22:02<08:09,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 79%|███████▉  | 681/864 [22:03<06:36,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 79%|███████▉  | 682/864 [22:04<05:47,  1.91s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 79%|███████▉  | 683/864 [22:05<05:06,  1.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 79%|███████▉  | 684/864 [22:06<04:30,  1.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 79%|███████▉  | 685/864 [22:08<04:41,  1.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 79%|███████▉  | 686/864 [22:09<04:37,  1.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 80%|███████▉  | 687/864 [22:11<04:17,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 80%|███████▉  | 688/864 [22:12<03:51,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 80%|███████▉  | 689/864 [22:13<03:42,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 80%|███████▉  | 690/864 [22:14<03:46,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 80%|███████▉  | 691/864 [22:15<03:44,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 80%|████████  | 692/864 [22:17<03:56,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 80%|████████  | 693/864 [22:18<03:35,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 80%|████████  | 694/864 [22:30<12:26,  4.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 80%|████████  | 695/864 [22:31<09:35,  3.40s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 81%|████████  | 696/864 [22:32<07:41,  2.75s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 81%|████████  | 697/864 [22:33<06:12,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 81%|████████  | 698/864 [22:34<05:29,  1.98s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 81%|████████  | 699/864 [22:37<06:09,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 81%|████████  | 700/864 [22:38<05:10,  1.89s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 81%|████████  | 701/864 [22:39<04:30,  1.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 81%|████████▏ | 702/864 [22:42<05:21,  1.98s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 81%|████████▏ | 703/864 [22:45<06:00,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 81%|████████▏ | 704/864 [22:46<05:09,  1.94s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 82%|████████▏ | 705/864 [22:48<04:43,  1.78s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 82%|████████▏ | 706/864 [22:49<04:26,  1.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 82%|████████▏ | 707/864 [22:50<03:57,  1.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 82%|████████▏ | 708/864 [22:51<03:37,  1.40s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 82%|████████▏ | 709/864 [23:02<11:07,  4.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 82%|████████▏ | 710/864 [23:04<08:50,  3.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 82%|████████▏ | 711/864 [23:05<07:09,  2.80s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 82%|████████▏ | 712/864 [23:07<06:03,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 83%|████████▎ | 713/864 [23:08<05:02,  2.01s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 83%|████████▎ | 714/864 [23:09<04:28,  1.79s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 83%|████████▎ | 715/864 [23:10<04:01,  1.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 83%|████████▎ | 716/864 [23:12<03:45,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 83%|████████▎ | 717/864 [23:13<03:24,  1.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 83%|████████▎ | 718/864 [23:14<03:12,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 83%|████████▎ | 719/864 [23:15<03:15,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 83%|████████▎ | 720/864 [23:16<02:57,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 83%|████████▎ | 721/864 [23:17<02:58,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 84%|████████▎ | 722/864 [23:19<02:53,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 84%|████████▎ | 723/864 [23:20<03:04,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 84%|████████▍ | 724/864 [23:21<02:57,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 84%|████████▍ | 725/864 [23:23<03:31,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 84%|████████▍ | 726/864 [23:24<03:08,  1.36s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 84%|████████▍ | 727/864 [23:26<02:57,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 84%|████████▍ | 728/864 [23:27<02:56,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 84%|████████▍ | 729/864 [23:28<02:48,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 84%|████████▍ | 730/864 [23:29<02:36,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 85%|████████▍ | 731/864 [23:30<02:34,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 85%|████████▍ | 732/864 [23:41<08:51,  4.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 85%|████████▍ | 733/864 [23:42<06:53,  3.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 85%|████████▍ | 734/864 [23:43<05:37,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 85%|████████▌ | 735/864 [23:45<04:55,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 85%|████████▌ | 736/864 [23:46<04:15,  2.00s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 85%|████████▌ | 737/864 [23:48<04:00,  1.89s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 85%|████████▌ | 738/864 [23:49<03:34,  1.70s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 86%|████████▌ | 739/864 [23:50<03:06,  1.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 86%|████████▌ | 740/864 [23:51<02:58,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 86%|████████▌ | 741/864 [23:53<02:48,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 86%|████████▌ | 742/864 [23:54<02:49,  1.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 86%|████████▌ | 743/864 [23:55<02:36,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 86%|████████▌ | 744/864 [23:56<02:31,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 86%|████████▌ | 745/864 [23:57<02:28,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 86%|████████▋ | 746/864 [24:09<08:17,  4.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 86%|████████▋ | 747/864 [24:10<06:26,  3.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 87%|████████▋ | 748/864 [24:11<05:00,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 87%|████████▋ | 749/864 [24:12<04:27,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 87%|████████▋ | 750/864 [24:14<03:53,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 87%|████████▋ | 751/864 [24:15<03:15,  1.73s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 87%|████████▋ | 752/864 [24:16<03:07,  1.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 87%|████████▋ | 753/864 [24:18<02:49,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 87%|████████▋ | 754/864 [24:19<02:38,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 87%|████████▋ | 755/864 [24:20<02:25,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 88%|████████▊ | 756/864 [24:21<02:12,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 88%|████████▊ | 757/864 [24:22<02:07,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 88%|████████▊ | 758/864 [24:23<02:11,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 88%|████████▊ | 759/864 [24:24<02:07,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 88%|████████▊ | 760/864 [24:26<02:15,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 88%|████████▊ | 761/864 [24:27<02:13,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 88%|████████▊ | 762/864 [24:28<02:09,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 88%|████████▊ | 763/864 [24:30<02:04,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 88%|████████▊ | 764/864 [24:31<01:59,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 89%|████████▊ | 765/864 [24:32<01:57,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 89%|████████▊ | 766/864 [24:33<02:03,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 89%|████████▉ | 767/864 [24:34<01:57,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 89%|████████▉ | 768/864 [24:36<02:03,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 89%|████████▉ | 769/864 [24:38<02:30,  1.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 89%|████████▉ | 770/864 [24:39<02:13,  1.42s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 89%|████████▉ | 771/864 [24:40<02:00,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 89%|████████▉ | 772/864 [24:42<02:02,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 89%|████████▉ | 773/864 [24:43<02:01,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 90%|████████▉ | 774/864 [24:44<01:53,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 90%|████████▉ | 775/864 [24:45<01:43,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 90%|████████▉ | 776/864 [24:46<01:46,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 90%|████████▉ | 777/864 [24:47<01:41,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 90%|█████████ | 778/864 [24:48<01:36,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 90%|█████████ | 779/864 [24:59<05:39,  3.99s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 90%|█████████ | 780/864 [25:00<04:20,  3.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 90%|█████████ | 781/864 [25:11<07:26,  5.38s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 91%|█████████ | 782/864 [25:12<05:42,  4.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 91%|█████████ | 783/864 [25:13<04:23,  3.26s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 91%|█████████ | 784/864 [25:14<03:32,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 91%|█████████ | 785/864 [25:16<03:03,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 91%|█████████ | 786/864 [25:17<02:36,  2.01s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 91%|█████████ | 787/864 [25:19<02:20,  1.83s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 91%|█████████ | 788/864 [25:20<02:06,  1.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 91%|█████████▏| 789/864 [25:21<01:54,  1.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 91%|█████████▏| 790/864 [25:23<01:49,  1.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 92%|█████████▏| 791/864 [25:34<05:16,  4.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 92%|█████████▏| 792/864 [25:35<04:04,  3.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 92%|█████████▏| 793/864 [25:36<03:14,  2.74s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 92%|█████████▏| 794/864 [25:37<02:36,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 92%|█████████▏| 795/864 [25:38<02:14,  1.94s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 92%|█████████▏| 796/864 [25:40<02:14,  1.98s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 92%|█████████▏| 797/864 [25:41<01:51,  1.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 92%|█████████▏| 798/864 [25:42<01:39,  1.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 92%|█████████▏| 799/864 [25:44<01:29,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 93%|█████████▎| 800/864 [25:45<01:28,  1.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 93%|█████████▎| 801/864 [25:46<01:23,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 93%|█████████▎| 802/864 [25:49<01:42,  1.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 93%|█████████▎| 803/864 [25:50<01:31,  1.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 93%|█████████▎| 804/864 [25:51<01:24,  1.41s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 93%|█████████▎| 805/864 [25:52<01:22,  1.40s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 93%|█████████▎| 806/864 [25:54<01:20,  1.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 93%|█████████▎| 807/864 [25:55<01:14,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 94%|█████████▎| 808/864 [25:56<01:09,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 94%|█████████▎| 809/864 [26:07<03:48,  4.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 94%|█████████▍| 810/864 [26:08<02:57,  3.29s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 94%|█████████▍| 811/864 [26:09<02:19,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 94%|█████████▍| 812/864 [26:10<01:54,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 94%|█████████▍| 813/864 [26:11<01:35,  1.86s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 94%|█████████▍| 814/864 [26:13<01:25,  1.72s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 94%|█████████▍| 815/864 [26:14<01:11,  1.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 94%|█████████▍| 816/864 [26:15<01:03,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 95%|█████████▍| 817/864 [26:16<01:00,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 95%|█████████▍| 818/864 [26:17<00:58,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 95%|█████████▍| 819/864 [26:18<00:53,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 95%|█████████▍| 820/864 [26:19<00:50,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 95%|█████████▌| 821/864 [26:21<00:52,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 95%|█████████▌| 822/864 [26:22<00:59,  1.41s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 95%|█████████▌| 823/864 [26:25<01:10,  1.72s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 95%|█████████▌| 824/864 [26:26<01:00,  1.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 95%|█████████▌| 825/864 [26:27<00:56,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 96%|█████████▌| 826/864 [26:28<00:52,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 96%|█████████▌| 827/864 [26:29<00:47,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 96%|█████████▌| 828/864 [26:31<00:46,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 96%|█████████▌| 829/864 [26:32<00:44,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 96%|█████████▌| 830/864 [26:34<00:47,  1.40s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 96%|█████████▌| 831/864 [26:35<00:45,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 96%|█████████▋| 832/864 [26:38<00:58,  1.83s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 96%|█████████▋| 833/864 [26:39<00:51,  1.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 97%|█████████▋| 834/864 [26:40<00:43,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 97%|█████████▋| 835/864 [26:41<00:38,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 97%|█████████▋| 836/864 [26:42<00:34,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 97%|█████████▋| 837/864 [26:53<01:53,  4.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 97%|█████████▋| 838/864 [27:04<02:40,  6.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 97%|█████████▋| 839/864 [27:05<01:56,  4.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 97%|█████████▋| 840/864 [27:06<01:27,  3.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 97%|█████████▋| 841/864 [27:08<01:07,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 97%|█████████▋| 842/864 [27:09<00:51,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 98%|█████████▊| 843/864 [27:10<00:42,  2.00s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 98%|█████████▊| 844/864 [27:11<00:36,  1.80s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 98%|█████████▊| 845/864 [27:12<00:30,  1.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 98%|█████████▊| 846/864 [27:14<00:26,  1.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 98%|█████████▊| 847/864 [27:15<00:23,  1.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 98%|█████████▊| 848/864 [27:16<00:23,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 98%|█████████▊| 849/864 [27:17<00:19,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 98%|█████████▊| 850/864 [27:29<01:00,  4.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 98%|█████████▊| 851/864 [27:30<00:44,  3.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 99%|█████████▊| 852/864 [27:31<00:32,  2.72s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 99%|█████████▊| 853/864 [27:32<00:24,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 99%|█████████▉| 854/864 [27:33<00:19,  1.95s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 99%|█████████▉| 855/864 [27:35<00:15,  1.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 99%|█████████▉| 856/864 [27:36<00:12,  1.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 99%|█████████▉| 857/864 [27:37<00:09,  1.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 99%|█████████▉| 858/864 [27:47<00:24,  4.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 99%|█████████▉| 859/864 [27:48<00:16,  3.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "100%|█████████▉| 860/864 [27:51<00:12,  3.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "100%|█████████▉| 861/864 [27:52<00:07,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "100%|█████████▉| 862/864 [27:53<00:03,  1.98s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "100%|█████████▉| 863/864 [27:54<00:01,  1.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "100%|██████████| 864/864 [27:55<00:00,  1.94s/it]\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T18:09:22.277040Z",
     "start_time": "2024-12-15T18:09:22.269548Z"
    }
   },
   "cell_type": "code",
   "source": "pre_sft_metrics",
   "id": "c082076cbd1b2f34",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': np.float64(0.002546296296296296),\n",
       " 'NDCG@k': np.float64(0.002130172178852872),\n",
       " 'Precision@k': 0.0025462962962962965,\n",
       " 'Recall@k': 0.0022872574955908292}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T18:38:33.216880Z",
     "start_time": "2024-12-15T18:09:22.340435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sft_model_id = \"./models/sft-1m\"\n",
    "sft_metrics = evaluate_model(sft_model_id, k=5)"
   ],
   "id": "3fbeece9eb432aa3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 37/864 [01:17<28:55,  2.10s/it]/home/rishabh-project/miniconda3/envs/LLM-RL4Rec-conda-env/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:3904: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/rishabh-project/miniconda3/envs/LLM-RL4Rec-conda-env/lib/python3.12/site-packages/numpy/_core/_methods.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 864/864 [28:56<00:00,  2.01s/it] \n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T18:38:33.269950Z",
     "start_time": "2024-12-15T18:38:33.264085Z"
    }
   },
   "cell_type": "code",
   "source": "sft_metrics",
   "id": "fcd415ca23f518ad",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': np.float64(nan),\n",
       " 'NDCG@k': np.float64(0.0234145817875562),\n",
       " 'Precision@k': 0.02199074074074074,\n",
       " 'Recall@k': 0.018121693121693122}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "model_id = \"./models/ppo_trained_model\"  # Replace with the identified base model\n",
    "adapter_dir = \"./models/sft\"   # Path to your adapter\n",
    "\n",
    "ppo_model = AutoModelForCausalLM.from_pretrained(\n",
    "    adapter_dir,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "ppo_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "ppo_tokenizer.pad_token = ppo_tokenizer.eos_token"
   ],
   "id": "35f1b57f8301519e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T19:34:48.946463Z",
     "start_time": "2024-12-17T19:29:58.901651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "k=5\n",
    "log_filename = \"ppo_evaluation.jsonl\"  # Use a JSONL file for structured logging\n",
    "logger = logging.getLogger(model_id)\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.FileHandler(log_filename, mode='w')\n",
    "handler.setFormatter(logging.Formatter('%(message)s'))\n",
    "logger.addHandler(handler)\n",
    "\n",
    "def log_step(data):\n",
    "    \"\"\"Log each step as a structured JSON object.\"\"\"\n",
    "    logger.info(json.dumps(data))\n",
    "\n",
    "log_step({\"event\": \"evaluation_start\", \"model_id\": model_id})\n",
    "\n",
    "# # Load model and tokenizer\n",
    "# test_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     torch_dtype=torch.float16,\n",
    "# ).to(\"cuda\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id, device=\"cuda\")\n",
    "pipe = pipeline(\"text-generation\", model=ppo_model, tokenizer=ppo_tokenizer)\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "for example in tqdm(test_set):\n",
    "    prompt = example['prompt'] + additional_instruction\n",
    "    ground_truth = [item.strip() for item in example['completion'].split(\",\")]\n",
    "\n",
    "    # Generate recommendations\n",
    "    message = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    outputs = pipe(message, max_new_tokens=256)\n",
    "    response = outputs[0]['generated_text'][-1]['content']\n",
    "    llm_generated_movies = extract_movie_titles(response)\n",
    "\n",
    "    # Match generated movies to actual movie titles\n",
    "    matched_titles_with_scores = match_titles_batch(llm_generated_movies, all_movies)\n",
    "    recommended_movies = [title for title, score in matched_titles_with_scores if title is not None]\n",
    "\n",
    "    # Evaluate recommendations\n",
    "    metrics = evaluate_recommendations(ground_truth, recommended_movies, k)\n",
    "    all_metrics.append(metrics)\n",
    "\n",
    "    # Log details for this example\n",
    "    log_step({\n",
    "        \"prompt\": prompt,\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"llm_response\": response,\n",
    "        \"llm_output\": llm_generated_movies,\n",
    "        \"matched_titles_with_scores\": matched_titles_with_scores,\n",
    "        \"recommended_movies\": recommended_movies,\n",
    "        \"metrics\": metrics,\n",
    "    })\n",
    "\n",
    "# Calculate and log average metrics\n",
    "avg_metrics = {metric: sum(m[metric] for m in all_metrics) / len(all_metrics) for metric in all_metrics[0]}\n",
    "log_step({\"event\": \"evaluation_end\", \"model_id\": model_id, \"average_metrics\": avg_metrics})\n",
    "\n",
    "print(avg_metrics)"
   ],
   "id": "d8bf87f7cd7ca7f3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 10/132 [00:21<04:37,  2.28s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|██████████| 132/132 [04:50<00:00,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAP': np.float64(0.01818181818181818), 'NDCG@k': np.float64(0.0166918513294695), 'Precision@k': 0.01818181818181818, 'Recall@k': 0.015593434343434341}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T18:51:17.914367Z",
     "start_time": "2024-12-17T18:51:11.195918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ppo_model_id = \"./models/ppo_trained_model\"\n",
    "# ppo_metrics = evaluate_model(ppo_model_id, k=5)"
   ],
   "id": "f5c139c81fae5fa0",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LoraConfig.__init__() got an unexpected keyword argument 'eva_config'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m ppo_model_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./models/ppo_trained_model\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 2\u001B[0m ppo_metrics \u001B[38;5;241m=\u001B[39m evaluate_model(ppo_model_id, k\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m)\n",
      "Cell \u001B[0;32mIn[25], line 22\u001B[0m, in \u001B[0;36mevaluate_model\u001B[0;34m(model_id, k)\u001B[0m\n\u001B[1;32m     19\u001B[0m log_step({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mevent\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mevaluation_start\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_id\u001B[39m\u001B[38;5;124m\"\u001B[39m: model_id})\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Load model and tokenizer\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m test_model \u001B[38;5;241m=\u001B[39m AutoModelForCausalLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[1;32m     23\u001B[0m     model_id,\n\u001B[1;32m     24\u001B[0m     torch_dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat16,\n\u001B[1;32m     25\u001B[0m )\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     26\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_id, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     27\u001B[0m pipe \u001B[38;5;241m=\u001B[39m pipeline(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext-generation\u001B[39m\u001B[38;5;124m\"\u001B[39m, model\u001B[38;5;241m=\u001B[39mtest_model, tokenizer\u001B[38;5;241m=\u001B[39mtokenizer, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/LLM-RL4Rec-conda-env/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:564\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    562\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m    563\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n\u001B[0;32m--> 564\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[1;32m    565\u001B[0m         pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39mmodel_args, config\u001B[38;5;241m=\u001B[39mconfig, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    566\u001B[0m     )\n\u001B[1;32m    567\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    568\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    569\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    570\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/envs/LLM-RL4Rec-conda-env/lib/python3.12/site-packages/transformers/modeling_utils.py:4310\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   4307\u001B[0m     model\u001B[38;5;241m.\u001B[39mhf_quantizer \u001B[38;5;241m=\u001B[39m hf_quantizer\n\u001B[1;32m   4309\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _adapter_model_path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 4310\u001B[0m     model\u001B[38;5;241m.\u001B[39mload_adapter(\n\u001B[1;32m   4311\u001B[0m         _adapter_model_path,\n\u001B[1;32m   4312\u001B[0m         adapter_name\u001B[38;5;241m=\u001B[39madapter_name,\n\u001B[1;32m   4313\u001B[0m         token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[1;32m   4314\u001B[0m         adapter_kwargs\u001B[38;5;241m=\u001B[39madapter_kwargs,\n\u001B[1;32m   4315\u001B[0m     )\n\u001B[1;32m   4317\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_loading_info:\n\u001B[1;32m   4318\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m loading_info \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniconda3/envs/LLM-RL4Rec-conda-env/lib/python3.12/site-packages/transformers/integrations/peft.py:207\u001B[0m, in \u001B[0;36mPeftAdapterMixin.load_adapter\u001B[0;34m(self, peft_model_id, adapter_name, revision, token, device_map, max_memory, offload_folder, offload_index, peft_config, adapter_state_dict, low_cpu_mem_usage, adapter_kwargs)\u001B[0m\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m adapter_config_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    202\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    203\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124madapter model file not found in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpeft_model_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Make sure you are passing the correct path to the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    204\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124madapter model.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    205\u001B[0m         )\n\u001B[0;32m--> 207\u001B[0m     peft_config \u001B[38;5;241m=\u001B[39m PeftConfig\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[1;32m    208\u001B[0m         peft_model_id,\n\u001B[1;32m    209\u001B[0m         token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[1;32m    210\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39madapter_kwargs,\n\u001B[1;32m    211\u001B[0m     )\n\u001B[1;32m    213\u001B[0m \u001B[38;5;66;03m# Create and add fresh new adapters into the model.\u001B[39;00m\n\u001B[1;32m    214\u001B[0m inject_adapter_in_model(peft_config, \u001B[38;5;28mself\u001B[39m, adapter_name, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpeft_load_kwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/LLM-RL4Rec-conda-env/lib/python3.12/site-packages/peft/config.py:152\u001B[0m, in \u001B[0;36mPeftConfigMixin.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, subfolder, **kwargs)\u001B[0m\n\u001B[1;32m    150\u001B[0m loaded_attributes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mfrom_json_file(config_file)\n\u001B[1;32m    151\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mclass_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mloaded_attributes}\n\u001B[0;32m--> 152\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mfrom_peft_type(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/LLM-RL4Rec-conda-env/lib/python3.12/site-packages/peft/config.py:119\u001B[0m, in \u001B[0;36mPeftConfigMixin.from_peft_type\u001B[0;34m(cls, **kwargs)\u001B[0m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    117\u001B[0m     config_cls \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\n\u001B[0;32m--> 119\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m config_cls(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "\u001B[0;31mTypeError\u001B[0m: LoraConfig.__init__() got an unexpected keyword argument 'eva_config'"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "ppo_metrics",
   "id": "1d7abb6e8c1c941a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T19:10:29.809092Z",
     "start_time": "2024-12-16T19:10:29.803891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "import json\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm"
   ],
   "id": "b89b477a7e3a1ac2",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T19:35:26.403185Z",
     "start_time": "2024-12-16T19:35:26.392373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tr_data = dataset['train']\n",
    "# test_set_reduced = test_set.select(range(10))\n",
    "tr_subset = tr_data.select(range(9,11))\n",
    "len(tr_data)"
   ],
   "id": "17e809e112d96502",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4030"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T19:37:25.111282Z",
     "start_time": "2024-12-16T19:37:25.097617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_and_evaluate_samples(model_id, dataset, start=0, end=4000, k=5, metric=\"Recall@k\"):\n",
    "    \"\"\"\n",
    "    Generate k samples for each prompt and evaluate their metrics.\n",
    "    \"\"\"\n",
    "    # Load model and tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        ).to(\"cuda\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, device=\"cuda\")\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=\"cuda\")\n",
    "\n",
    "    preferences = []\n",
    "    for data in tqdm(dataset[\"train\"].select(range(start,end)), desc=f\"Generating samples and evaluating metrics from {start} to {end}\"):\n",
    "        prompt = data['prompt']  + additional_instruction\n",
    "        ground_truth = [item.strip() for item in data['completion'].split(\",\")]\n",
    "\n",
    "        # Generate k responses\n",
    "        responses = []\n",
    "        chosen_metrics = []\n",
    "        for _ in range(k):\n",
    "            # Generate recommendations\n",
    "            message = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            outputs = pipe(message, max_new_tokens=256)\n",
    "            response = outputs[0]['generated_text'][-1]['content']\n",
    "            llm_generated_movies = extract_movie_titles(response)\n",
    "\n",
    "            responses.append(response)\n",
    "\n",
    "            # Evaluate the response\n",
    "            # Match generated movies to actual movie titles\n",
    "            matched_titles_with_scores = match_titles_batch(llm_generated_movies, all_movies)\n",
    "            recommended_movies = [title for title, score in matched_titles_with_scores if title is not None]\n",
    "\n",
    "            # Evaluate recommendations\n",
    "            metrics = evaluate_recommendations(ground_truth, recommended_movies, k)\n",
    "            # metric_value = evaluate_recommendations(ground_truth, response_text, metric)\n",
    "            chosen_metrics.append(metrics[metric])\n",
    "\n",
    "        # Store results\n",
    "        preferences.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"responses\": responses,\n",
    "            \"metrics\": chosen_metrics\n",
    "        })\n",
    "\n",
    "    return preferences"
   ],
   "id": "de986371ad1a30e5",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T19:36:55.558361Z",
     "start_time": "2024-12-16T19:36:55.548016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_pairwise_preferences(preferences, metric=\"Recall@k\"):\n",
    "    \"\"\"\n",
    "    Generate pairwise preference dataset based on metric comparisons.\n",
    "    \"\"\"\n",
    "    pairwise_data = []\n",
    "    for item in tqdm(preferences, desc=\"Generating pairwise preferences\"):\n",
    "        prompt = item[\"prompt\"]\n",
    "        responses = item[\"responses\"]\n",
    "        metrics = item[\"metrics\"]\n",
    "\n",
    "        # Generate all combinations of responses (kC2)\n",
    "        for (idx1, idx2) in itertools.combinations(range(len(responses)), 2):\n",
    "            response_1 = responses[idx1]\n",
    "            response_2 = responses[idx2]\n",
    "            metric_1 = metrics[idx1]\n",
    "            metric_2 = metrics[idx2]\n",
    "\n",
    "            # Compare metrics and assign preference\n",
    "            if metric_1 > metric_2:\n",
    "                pairwise_data.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"response_1\": response_1,\n",
    "                    \"response_2\": response_2,\n",
    "                    \"preferred\": \"response_1\"\n",
    "                })\n",
    "            elif metric_2 > metric_1:\n",
    "                pairwise_data.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"response_1\": response_1,\n",
    "                    \"response_2\": response_2,\n",
    "                    \"preferred\": \"response_2\"\n",
    "                })\n",
    "\n",
    "    return pairwise_data"
   ],
   "id": "19fa89fa1dddfb86",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-16T19:47:29.071961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parameters\n",
    "sft_model_id = \"./models/sft-1m\"\n",
    "k = 5\n",
    "metric = \"NDCG@k\"\n",
    "start,end = 0,1000\n",
    "# Step 1: Generate samples and evaluate metrics\n",
    "print(\"Step 1: Generating and evaluating samples...\")\n",
    "preferences = generate_and_evaluate_samples(sft_model_id, dataset, start, end, k, metric)"
   ],
   "id": "79a950939322defb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Generating and evaluating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples and evaluating metrics from 0 to 1000:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T19:31:11.748427Z",
     "start_time": "2024-12-16T19:31:11.743587Z"
    }
   },
   "cell_type": "code",
   "source": "# preferences",
   "id": "386a9cf63b977d9a",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T19:47:21.042754269Z",
     "start_time": "2024-12-02T02:17:09.131600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"raw_preferences-1m_1.jsonl\", \"w\") as f:\n",
    "    for entry in preferences:\n",
    "        f.write(json.dumps(entry) + \"\\n\")"
   ],
   "id": "dcb5163002fbc4f1",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Parameters\n",
    "sft_model_id = \"./models/sft-1m\"\n",
    "k = 5\n",
    "metric = \"NDCG@k\"\n",
    "start,end = 1000,2000\n",
    "\n",
    "# Step 1: Generate samples and evaluate metrics\n",
    "print(\"Step 1: Generating and evaluating samples...\")\n",
    "preferences = generate_and_evaluate_samples(sft_model_id, dataset, start, end, k, metric)"
   ],
   "id": "6ede4b5b41787af6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with open(\"raw_preferences-1m_2.jsonl\", \"w\") as f:\n",
    "    for entry in preferences:\n",
    "        f.write(json.dumps(entry) + \"\\n\")"
   ],
   "id": "db5156e7e96fcd6f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T02:17:09.424574Z",
     "start_time": "2024-12-02T02:17:09.367391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 2: Generate pairwise preference dataset\n",
    "print(\"Step 2: Generating pairwise preferences...\")\n",
    "pairwise_preferences = generate_pairwise_preferences(preferences, metric)\n",
    "\n",
    "# Save results to file\n",
    "with open(\"preferences-1m.jsonl\", \"w\") as f:\n",
    "    for entry in pairwise_preferences:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(\"Pairwise preferences generated and saved to 'preferences-1m.jsonl'.\")"
   ],
   "id": "6d29566d71471367",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Generating pairwise preferences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairwise preferences: 100%|██████████| 611/611 [00:00<00:00, 55361.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairwise preferences generated and saved to 'preferences.jsonl'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T18:51:18.915669Z",
     "start_time": "2024-12-02T18:51:18.853513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "def parse_preferences(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Parses the preferences.jsonl file and converts it into the desired format.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Path to the preferences.jsonl file.\n",
    "        output_file (str): Path to save the parsed output file.\n",
    "    \"\"\"\n",
    "    parsed_data = []\n",
    "\n",
    "    # Read the JSONL file\n",
    "    with open(input_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            # Parse each line as JSON\n",
    "            data = json.loads(line.strip())\n",
    "            prompt = data[\"prompt\"]\n",
    "            response_1 = data[\"response_1\"]\n",
    "            response_2 = data[\"response_2\"]\n",
    "            preferred = data[\"preferred\"]\n",
    "\n",
    "            # Determine chosen and rejected based on preference\n",
    "            if preferred == \"response_1\":\n",
    "                chosen = response_1\n",
    "                rejected = response_2\n",
    "            elif preferred == \"response_2\":\n",
    "                chosen = response_2\n",
    "                rejected = response_1\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid preferred value: {preferred}\")\n",
    "\n",
    "            # Add to parsed data\n",
    "            parsed_data.append({\n",
    "                \"prompt\": prompt,\n",
    "                \"chosen\": chosen,\n",
    "                \"rejected\": rejected\n",
    "            })\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(parsed_data, f)\n",
    "\n",
    "    print(f\"Parsed data saved to {output_file}\")\n",
    "    return parsed_data\n",
    "\n",
    "\n",
    "input_file = \"preferences-1m.jsonl\"  # Path to your input file\n",
    "output_file = \"parsed_preferences-1m.json\"  # Path to save the parsed data\n",
    "parsed_data = parse_preferences(input_file, output_file)"
   ],
   "id": "4800b3a2e9691990",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed data saved to parsed_preferences.json\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T18:53:42.229147Z",
     "start_time": "2024-12-02T18:53:42.209787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "preference_data_file_name = \"./parsed_preferences.json\"\n",
    "with open(preference_data_file_name, \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "#dataset = load_dataset(\"json\", data_files=\"preferences.jsonl\",split=\"train\")\n"
   ],
   "id": "79c89224af613e4c",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T00:21:21.107567Z",
     "start_time": "2024-12-03T00:21:19.669435Z"
    }
   },
   "cell_type": "code",
   "source": "!python --version",
   "id": "486f4c224a01291a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.10\r\n"
     ]
    }
   ],
   "execution_count": 138
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T00:09:57.819215Z",
     "start_time": "2024-12-03T00:09:39.786955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install transformers\"<=4.45\"\n",
    "!pip install accelerate\n",
    "!pip install tokenizers\n"
   ],
   "id": "c757ef47eb60e0e2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers<=4.45 in ./.venv/lib/python3.8/site-packages (4.41.0)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.8/site-packages (from transformers<=4.45) (3.16.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in ./.venv/lib/python3.8/site-packages (from transformers<=4.45) (0.26.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.8/site-packages (from transformers<=4.45) (1.24.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.8/site-packages (from transformers<=4.45) (24.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.8/site-packages (from transformers<=4.45) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.8/site-packages (from transformers<=4.45) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.8/site-packages (from transformers<=4.45) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.venv/lib/python3.8/site-packages (from transformers<=4.45) (0.19.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.8/site-packages (from transformers<=4.45) (0.4.5)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.8/site-packages (from transformers<=4.45) (4.67.0)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers<=4.45) (2024.9.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers<=4.45) (4.12.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.8/site-packages (from requests->transformers<=4.45) (3.4.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.8/site-packages (from requests->transformers<=4.45) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.8/site-packages (from requests->transformers<=4.45) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.8/site-packages (from requests->transformers<=4.45) (2024.8.30)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in ./.venv/lib/python3.8/site-packages (0.30.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.8/site-packages (from accelerate) (1.24.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.8/site-packages (from accelerate) (24.2)\r\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.8/site-packages (from accelerate) (6.1.0)\r\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.8/site-packages (from accelerate) (6.0.2)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in ./.venv/lib/python3.8/site-packages (from accelerate) (2.4.1)\r\n",
      "Requirement already satisfied: huggingface-hub in ./.venv/lib/python3.8/site-packages (from accelerate) (0.26.1)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.venv/lib/python3.8/site-packages (from accelerate) (0.4.5)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.16.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2024.9.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.venv/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2.20.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\r\n",
      "Requirement already satisfied: triton==3.0.0 in ./.venv/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.0.0)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.6.85)\r\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.8/site-packages (from huggingface-hub->accelerate) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.8/site-packages (from huggingface-hub->accelerate) (4.67.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.8/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (3.4.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (2024.8.30)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.8/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in ./.venv/lib/python3.8/site-packages (0.19.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./.venv/lib/python3.8/site-packages (from tokenizers) (0.26.1)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.16.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.9.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\r\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.8/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.8/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.8/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.8/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.8.30)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: trl in ./.venv/lib/python3.8/site-packages (0.11.4)\r\n",
      "Requirement already satisfied: torch>=1.4.0 in ./.venv/lib/python3.8/site-packages (from trl) (2.4.1)\r\n",
      "Requirement already satisfied: transformers>=4.40.0 in ./.venv/lib/python3.8/site-packages (from trl) (4.41.0)\r\n",
      "Requirement already satisfied: accelerate in ./.venv/lib/python3.8/site-packages (from trl) (0.30.1)\r\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.8/site-packages (from trl) (3.1.0)\r\n",
      "Requirement already satisfied: tyro>=0.5.11 in ./.venv/lib/python3.8/site-packages (from trl) (0.9.2)\r\n",
      "Requirement already satisfied: numpy>=1.18.2 in ./.venv/lib/python3.8/site-packages (from trl) (1.24.4)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (3.16.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (2024.9.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (2.20.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (12.1.105)\r\n",
      "Requirement already satisfied: triton==3.0.0 in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (3.0.0)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->trl) (12.6.85)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in ./.venv/lib/python3.8/site-packages (from transformers>=4.40.0->trl) (0.26.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.8/site-packages (from transformers>=4.40.0->trl) (24.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.8/site-packages (from transformers>=4.40.0->trl) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.8/site-packages (from transformers>=4.40.0->trl) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.8/site-packages (from transformers>=4.40.0->trl) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.venv/lib/python3.8/site-packages (from transformers>=4.40.0->trl) (0.19.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.8/site-packages (from transformers>=4.40.0->trl) (0.4.5)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.8/site-packages (from transformers>=4.40.0->trl) (4.67.0)\r\n",
      "Requirement already satisfied: docstring-parser>=0.16 in ./.venv/lib/python3.8/site-packages (from tyro>=0.5.11->trl) (0.16)\r\n",
      "Requirement already satisfied: eval-type-backport>=0.1.3 in ./.venv/lib/python3.8/site-packages (from tyro>=0.5.11->trl) (0.2.0)\r\n",
      "Requirement already satisfied: rich>=11.1.0 in ./.venv/lib/python3.8/site-packages (from tyro>=0.5.11->trl) (13.9.4)\r\n",
      "Requirement already satisfied: shtab>=1.5.6 in ./.venv/lib/python3.8/site-packages (from tyro>=0.5.11->trl) (1.7.1)\r\n",
      "Requirement already satisfied: typeguard>=4.0.0 in ./.venv/lib/python3.8/site-packages (from tyro>=0.5.11->trl) (4.4.0)\r\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.8/site-packages (from accelerate->trl) (6.1.0)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.8/site-packages (from datasets->trl) (17.0.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.8/site-packages (from datasets->trl) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.8/site-packages (from datasets->trl) (2.0.3)\r\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.8/site-packages (from datasets->trl) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.8/site-packages (from datasets->trl) (0.70.16)\r\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.8/site-packages (from datasets->trl) (3.10.11)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.8/site-packages (from aiohttp->datasets->trl) (2.4.3)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.8/site-packages (from aiohttp->datasets->trl) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.8/site-packages (from aiohttp->datasets->trl) (24.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.8/site-packages (from aiohttp->datasets->trl) (1.5.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.8/site-packages (from aiohttp->datasets->trl) (6.1.0)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./.venv/lib/python3.8/site-packages (from aiohttp->datasets->trl) (1.15.2)\r\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.venv/lib/python3.8/site-packages (from aiohttp->datasets->trl) (5.0.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.8/site-packages (from requests->transformers>=4.40.0->trl) (3.4.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.8/site-packages (from requests->transformers>=4.40.0->trl) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.8/site-packages (from requests->transformers>=4.40.0->trl) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.8/site-packages (from requests->transformers>=4.40.0->trl) (2024.8.30)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.8/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.8/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.18.0)\r\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in ./.venv/lib/python3.8/site-packages (from typeguard>=4.0.0->tyro>=0.5.11->trl) (8.5.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.8/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.8/site-packages (from pandas->datasets->trl) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.8/site-packages (from pandas->datasets->trl) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.venv/lib/python3.8/site-packages (from pandas->datasets->trl) (2024.2)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.8/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\r\n",
      "Requirement already satisfied: zipp>=3.20 in ./.venv/lib/python3.8/site-packages (from importlib-metadata>=3.6->typeguard>=4.0.0->tyro>=0.5.11->trl) (3.20.2)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.8/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.8/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets->trl) (0.2.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T00:21:08.069931Z",
     "start_time": "2024-12-03T00:21:03.221801Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install --upgrade trl",
   "id": "f0ec82646469a4ff",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: trl in ./.venv/lib/python3.8/site-packages (0.11.4)\r\n",
      "Requirement already satisfied: torch>=1.4.0 in ./.venv/lib/python3.8/site-packages (from trl) (2.4.1)\r\n",
      "Requirement already satisfied: transformers>=4.40.0 in ./.venv/lib/python3.8/site-packages (from trl) (4.41.0)\r\n",
      "Requirement already satisfied: accelerate in ./.venv/lib/python3.8/site-packages (from trl) (0.30.1)\r\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.8/site-packages (from trl) (3.1.0)\r\n",
      "Requirement already satisfied: tyro>=0.5.11 in ./.venv/lib/python3.8/site-packages (from trl) (0.9.2)\r\n",
      "Requirement already satisfied: numpy>=1.18.2 in ./.venv/lib/python3.8/site-packages (from trl) (1.24.4)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (3.16.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (2024.9.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (2.20.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (12.1.105)\r\n",
      "Requirement already satisfied: triton==3.0.0 in ./.venv/lib/python3.8/site-packages (from torch>=1.4.0->trl) (3.0.0)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->trl) (12.6.85)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in ./.venv/lib/python3.8/site-packages (from transformers>=4.40.0->trl) (0.26.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.8/site-packages (from transformers>=4.40.0->trl) (24.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.8/site-packages (from transformers>=4.40.0->trl) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.8/site-packages (from transformers>=4.40.0->trl) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.8/site-packages (from transformers>=4.40.0->trl) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.venv/lib/python3.8/site-packages (from transformers>=4.40.0->trl) (0.19.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.8/site-packages (from transformers>=4.40.0->trl) (0.4.5)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.8/site-packages (from transformers>=4.40.0->trl) (4.67.0)\r\n",
      "Requirement already satisfied: docstring-parser>=0.16 in ./.venv/lib/python3.8/site-packages (from tyro>=0.5.11->trl) (0.16)\r\n",
      "Requirement already satisfied: eval-type-backport>=0.1.3 in ./.venv/lib/python3.8/site-packages (from tyro>=0.5.11->trl) (0.2.0)\r\n",
      "Requirement already satisfied: rich>=11.1.0 in ./.venv/lib/python3.8/site-packages (from tyro>=0.5.11->trl) (13.9.4)\r\n",
      "Requirement already satisfied: shtab>=1.5.6 in ./.venv/lib/python3.8/site-packages (from tyro>=0.5.11->trl) (1.7.1)\r\n",
      "Requirement already satisfied: typeguard>=4.0.0 in ./.venv/lib/python3.8/site-packages (from tyro>=0.5.11->trl) (4.4.0)\r\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.8/site-packages (from accelerate->trl) (6.1.0)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.8/site-packages (from datasets->trl) (17.0.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.8/site-packages (from datasets->trl) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.8/site-packages (from datasets->trl) (2.0.3)\r\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.8/site-packages (from datasets->trl) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.8/site-packages (from datasets->trl) (0.70.16)\r\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.8/site-packages (from datasets->trl) (3.10.11)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.8/site-packages (from aiohttp->datasets->trl) (2.4.3)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.8/site-packages (from aiohttp->datasets->trl) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.8/site-packages (from aiohttp->datasets->trl) (24.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.8/site-packages (from aiohttp->datasets->trl) (1.5.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.8/site-packages (from aiohttp->datasets->trl) (6.1.0)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./.venv/lib/python3.8/site-packages (from aiohttp->datasets->trl) (1.15.2)\r\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.venv/lib/python3.8/site-packages (from aiohttp->datasets->trl) (5.0.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.8/site-packages (from requests->transformers>=4.40.0->trl) (3.4.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.8/site-packages (from requests->transformers>=4.40.0->trl) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.8/site-packages (from requests->transformers>=4.40.0->trl) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.8/site-packages (from requests->transformers>=4.40.0->trl) (2024.8.30)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.8/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.8/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.18.0)\r\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in ./.venv/lib/python3.8/site-packages (from typeguard>=4.0.0->tyro>=0.5.11->trl) (8.5.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.8/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.8/site-packages (from pandas->datasets->trl) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.8/site-packages (from pandas->datasets->trl) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.venv/lib/python3.8/site-packages (from pandas->datasets->trl) (2024.2)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.8/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\r\n",
      "Requirement already satisfied: zipp>=3.20 in ./.venv/lib/python3.8/site-packages (from importlib-metadata>=3.6->typeguard>=4.0.0->tyro>=0.5.11->trl) (3.20.2)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.8/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.8/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets->trl) (0.2.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 137
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T00:17:44.303674Z",
     "start_time": "2024-12-03T00:17:44.295926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig"
   ],
   "id": "7f5552b44c5ae12c",
   "outputs": [],
   "execution_count": 130
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T00:17:47.460506Z",
     "start_time": "2024-12-03T00:17:45.585814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dpo_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "dpo_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "dpo_tokenizer.pad_token = dpo_tokenizer.eos_token"
   ],
   "id": "b3ad11aacb857f91",
   "outputs": [],
   "execution_count": 131
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T00:17:47.782628Z",
     "start_time": "2024-12-03T00:17:47.474431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "preference_data_file_name = \"./parsed_preferences.json\"\n",
    "preference_dataset = load_dataset(\"json\", data_files=preference_data_file_name,split=\"train\")"
   ],
   "id": "ea89da76713b3a13",
   "outputs": [],
   "execution_count": 132
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T00:17:48.851328Z",
     "start_time": "2024-12-03T00:17:48.840629Z"
    }
   },
   "cell_type": "code",
   "source": "preference_dataset",
   "id": "302078859803f961",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'chosen', 'rejected'],\n",
       "    num_rows: 929\n",
       "})"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T00:17:53.653448Z",
     "start_time": "2024-12-03T00:17:53.132988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dpo_config = DPOConfig(\n",
    "    output_dir=\"./models/dpo_new\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "from peft import LoraConfig\n",
    "\n",
    "\n",
    "\n",
    "dpo_peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=\"all-linear\",\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ],
   "id": "29b04868847ef2a6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "execution_count": 134
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T00:18:01.403055Z",
     "start_time": "2024-12-03T00:17:55.962050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dpo_trainer = DPOTrainer(\n",
    "    model=dpo_model,\n",
    "    args=dpo_config,\n",
    "    train_dataset=preference_dataset,\n",
    "    tokenizer=dpo_tokenizer,\n",
    "    peft_config=dpo_peft_config\n",
    ")"
   ],
   "id": "35c674e0478648cb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use `Trainer.processing_class = processing_class` instead.\n",
      "/home/rishabh-project/PycharmProjects/LLM-RL4Rec/.venv/lib/python3.8/site-packages/trl/trainer/dpo_trainer.py:660: UserWarning: `max_length` is not set in the DPOConfig's init it will default to `512` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/home/rishabh-project/PycharmProjects/LLM-RL4Rec/.venv/lib/python3.8/site-packages/trl/trainer/dpo_trainer.py:673: UserWarning: `max_prompt_length` is not set in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/rishabh-project/PycharmProjects/LLM-RL4Rec/.venv/lib/python3.8/site-packages/trl/trainer/dpo_trainer.py:708: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/929 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "52370ae706564c9ab726931f0570fb91"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishabh-project/PycharmProjects/LLM-RL4Rec/.venv/lib/python3.8/site-packages/trl/trainer/dpo_trainer.py:822: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `DPOTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "execution_count": 135
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T00:18:02.029276Z",
     "start_time": "2024-12-03T00:18:01.414703Z"
    }
   },
   "cell_type": "code",
   "source": "dpo_trainer.train()",
   "id": "6a362954b5f1ca5f",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'generate'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[136], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdpo_trainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/LLM-RL4Rec/.venv/lib/python3.8/site-packages/transformers/trainer.py:2123\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   2121\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[1;32m   2122\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2123\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2124\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2125\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2126\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2127\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2128\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/LLM-RL4Rec/.venv/lib/python3.8/site-packages/transformers/trainer.py:2427\u001B[0m, in \u001B[0;36m_inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   2416\u001B[0m is_fsdp_ckpt \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misdir(resume_from_checkpoint) \u001B[38;5;129;01mand\u001B[39;00m (\n\u001B[1;32m   2417\u001B[0m     \u001B[38;5;66;03m# this checks the FSDP state dict when `SHARDED_STATE_DICT` is used\u001B[39;00m\n\u001B[1;32m   2418\u001B[0m     \u001B[38;5;28many\u001B[39m(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2424\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misfile(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(resume_from_checkpoint, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mFSDP_MODEL_NAME\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.bin\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m   2425\u001B[0m )\n\u001B[1;32m   2426\u001B[0m \u001B[38;5;66;03m# if multiple adapters exist, they get saved in sub directories\u001B[39;00m\n\u001B[0;32m-> 2427\u001B[0m adapter_subdirs \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   2428\u001B[0m     [\n\u001B[1;32m   2429\u001B[0m         folder_name\n\u001B[1;32m   2430\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m folder_name \u001B[38;5;129;01min\u001B[39;00m os\u001B[38;5;241m.\u001B[39mlistdir(resume_from_checkpoint)\n\u001B[1;32m   2431\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misdir(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(resume_from_checkpoint, folder_name))\n\u001B[1;32m   2432\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m (\n\u001B[1;32m   2433\u001B[0m             os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misfile(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(resume_from_checkpoint, folder_name, ADAPTER_WEIGHTS_NAME))\n\u001B[1;32m   2434\u001B[0m             \u001B[38;5;129;01mor\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misfile(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(resume_from_checkpoint, folder_name, ADAPTER_SAFE_WEIGHTS_NAME))\n\u001B[1;32m   2435\u001B[0m         )\n\u001B[1;32m   2436\u001B[0m     ]\n\u001B[1;32m   2437\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misdir(resume_from_checkpoint)\n\u001B[1;32m   2438\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m []\n\u001B[1;32m   2439\u001B[0m )\n\u001B[1;32m   2441\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_fsdp_ckpt \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_fsdp_enabled:\n\u001B[1;32m   2442\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCheckpoint found at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresume_from_checkpoint\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is only supported when using PyTorch FSDP\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/LLM-RL4Rec/.venv/lib/python3.8/site-packages/trl/trainer/dpo_trainer.py:1508\u001B[0m, in \u001B[0;36mDPOTrainer.get_batch_samples\u001B[0;34m(self, model, batch)\u001B[0m\n\u001B[1;32m   1505\u001B[0m generate_context_manager \u001B[38;5;241m=\u001B[39m amp\u001B[38;5;241m.\u001B[39mautocast(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_peft_has_been_casted_to_bf16 \u001B[38;5;28;01melse\u001B[39;00m nullcontext()\n\u001B[1;32m   1507\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m generate_context_manager:\n\u001B[0;32m-> 1508\u001B[0m     policy_output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m(\n\u001B[1;32m   1509\u001B[0m         input_ids\u001B[38;5;241m=\u001B[39mbatch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprompt_input_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m   1510\u001B[0m         attention_mask\u001B[38;5;241m=\u001B[39mbatch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprompt_attention_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m   1511\u001B[0m         max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_length,\n\u001B[1;32m   1512\u001B[0m         do_sample\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m   1513\u001B[0m         pad_token_id\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39mpad_token_id,\n\u001B[1;32m   1514\u001B[0m     )\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;66;03m# if reference_output in batch use that otherwise use the reference model\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreference_output\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m batch:\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'generator' object has no attribute 'generate'"
     ]
    }
   ],
   "execution_count": 136
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "784666952fc03296"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T18:58:32.520648Z",
     "start_time": "2024-12-02T18:58:27.073400Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install -U bitsandbytes",
   "id": "57a77eab36a89815",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in ./.venv/lib/python3.8/site-packages (0.44.1)\r\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.8/site-packages (from bitsandbytes) (2.4.1)\r\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.8/site-packages (from bitsandbytes) (1.24.4)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.8/site-packages (from torch->bitsandbytes) (3.16.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.8/site-packages (from torch->bitsandbytes) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.8/site-packages (from torch->bitsandbytes) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.8/site-packages (from torch->bitsandbytes) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.8/site-packages (from torch->bitsandbytes) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.8/site-packages (from torch->bitsandbytes) (2024.9.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.8/site-packages (from torch->bitsandbytes) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.8/site-packages (from torch->bitsandbytes) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.8/site-packages (from torch->bitsandbytes) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.8/site-packages (from torch->bitsandbytes) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.8/site-packages (from torch->bitsandbytes) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.8/site-packages (from torch->bitsandbytes) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.8/site-packages (from torch->bitsandbytes) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.8/site-packages (from torch->bitsandbytes) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.8/site-packages (from torch->bitsandbytes) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.venv/lib/python3.8/site-packages (from torch->bitsandbytes) (2.20.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.8/site-packages (from torch->bitsandbytes) (12.1.105)\r\n",
      "Requirement already satisfied: triton==3.0.0 in ./.venv/lib/python3.8/site-packages (from torch->bitsandbytes) (3.0.0)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.6.85)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.8/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.8/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T18:59:00.075032Z",
     "start_time": "2024-12-02T18:58:59.505851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, TrainingArguments, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from trl import DPOTrainer\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "#from utils import find_all_linear_names, print_trainable_parameters\n",
    "\n",
    "output_dir=\"./dpo_results\"\n",
    "model_name = \"./models/merged_peft/final_merged_checkpoint\"\n",
    "\n",
    "'''\n",
    "preference_data_file_name = \"./parsed_preferences.json\"\n",
    "with open(preference_data_file_name, \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "'''\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=preference_data_file_name,split=\"train\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, quantization_config=bnb_config)\n",
    "model.config.use_cache = False\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "model_ref = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, quantization_config=bnb_config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def return_prompt_and_responses(samples):\n",
    "    return {\n",
    "        \"prompt\": [\n",
    "            f\"An AI tool that corrects and rephrase user text grammar errors delimited by triple backticks to standard English.\\n### Input: ```{input}```\\n ### Output: \"\n",
    "            for input in samples[\"input\"]\n",
    "        ],\n",
    "        \"chosen\": samples[\"chosen\"],\n",
    "        \"rejected\": samples[\"rejected\"],\n",
    "    }\n",
    "\n",
    "original_columns = dataset.column_names\n",
    "\n",
    "dataset = dataset.map(\n",
    "    return_prompt_and_responses,\n",
    "    batched=True,\n",
    "    remove_columns=original_columns\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing =True,\n",
    "    max_grad_norm= 0.3,\n",
    "    num_train_epochs=15,\n",
    "    save_steps= 100,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=10,\n",
    "    output_dir=output_dir,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=16,\n",
    "    target_modules=find_all_linear_names(model),\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    model_ref,\n",
    "    args=training_args,\n",
    "    beta=0.1,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_prompt_length=1024,\n",
    "    max_length=2048,\n",
    ")\n",
    "\n",
    "\n",
    "dpo_trainer.train()\n",
    "dpo_trainer.save_model(output_dir)\n",
    "\n",
    "\n",
    "output_dir = os.path.join(output_dir, \"final_checkpoint\")\n",
    "dpo_trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ],
   "id": "acdca18dffb65d14",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[69], line 27\u001B[0m\n\u001B[1;32m     19\u001B[0m dataset \u001B[38;5;241m=\u001B[39m load_dataset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m, data_files\u001B[38;5;241m=\u001B[39mpreference_data_file_name,split\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     21\u001B[0m bnb_config \u001B[38;5;241m=\u001B[39m BitsAndBytesConfig(\n\u001B[1;32m     22\u001B[0m     load_in_4bit\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     23\u001B[0m     bnb_4bit_quant_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnf4\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     24\u001B[0m     bnb_4bit_compute_dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mbfloat16,\n\u001B[1;32m     25\u001B[0m )\n\u001B[0;32m---> 27\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbfloat16\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquantization_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbnb_config\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m model\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_cache \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     29\u001B[0m model \u001B[38;5;241m=\u001B[39m prepare_model_for_kbit_training(model)\n",
      "File \u001B[0;32m~/PycharmProjects/LLM-RL4Rec/.venv/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:564\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    562\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m    563\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n\u001B[0;32m--> 564\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    565\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    566\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    567\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    568\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    569\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    570\u001B[0m )\n",
      "File \u001B[0;32m~/PycharmProjects/LLM-RL4Rec/.venv/lib/python3.8/site-packages/transformers/modeling_utils.py:3657\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   3654\u001B[0m     hf_quantizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   3656\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m hf_quantizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 3657\u001B[0m     \u001B[43mhf_quantizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidate_environment\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3658\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_tf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfrom_tf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_flax\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfrom_flax\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\n\u001B[1;32m   3659\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3660\u001B[0m     torch_dtype \u001B[38;5;241m=\u001B[39m hf_quantizer\u001B[38;5;241m.\u001B[39mupdate_torch_dtype(torch_dtype)\n\u001B[1;32m   3661\u001B[0m     device_map \u001B[38;5;241m=\u001B[39m hf_quantizer\u001B[38;5;241m.\u001B[39mupdate_device_map(device_map)\n",
      "File \u001B[0;32m~/PycharmProjects/LLM-RL4Rec/.venv/lib/python3.8/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:74\u001B[0m, in \u001B[0;36mBnb4BitHfQuantizer.validate_environment\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[1;32m     71\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing `bitsandbytes` 4-bit quantization requires Accelerate: `pip install \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccelerate>=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mACCELERATE_MIN_VERSION\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     72\u001B[0m     )\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_bitsandbytes_available():\n\u001B[0;32m---> 74\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[1;32m     75\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     76\u001B[0m     )\n\u001B[1;32m     78\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mintegrations\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m validate_bnb_backend_availability\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m is_bitsandbytes_multi_backend_available\n",
      "\u001B[0;31mImportError\u001B[0m: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T17:31:35.274510Z",
     "start_time": "2024-12-15T17:31:14.496981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "def clear_all_memory():\n",
    "    \"\"\"Attempts to clear all memory (CPU and GPU).\"\"\"\n",
    "    # Clear PyTorch CUDA cache\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Run garbage collection\n",
    "    gc.collect()\n",
    "\n",
    "    # Clear variables from global namespace (use with caution!)\n",
    "    for name in dir():\n",
    "        if not name.startswith('_'):\n",
    "            del globals()[name]\n",
    "\n",
    "    # Clear modules from sys.modules (use with extreme caution!)\n",
    "    for name in list(sys.modules.keys()):\n",
    "        if name not in ['sys', 'gc', 'torch']:  # Preserve essential modules\n",
    "            del sys.modules[name]\n",
    "\n",
    "    # Run garbage collection again\n",
    "    gc.collect()\n",
    "\n",
    "# Call the function to attempt to clear memory\n",
    "clear_all_memory()"
   ],
   "id": "87a91b15419599ee",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T17:31:06.838066Z",
     "start_time": "2024-12-15T17:31:06.303624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def print_gpu_memory_usage():\n",
    "    \"\"\"Prints the memory usage of each GPU.\"\"\"\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}:\")\n",
    "        print(f\"  Total memory: {torch.cuda.get_device_properties(i).total_memory / (1024**3):.2f} GB\")\n",
    "        print(f\"  Allocated memory: {torch.cuda.memory_allocated(i) / (1024**3):.2f} GB\")\n",
    "        print(f\"  Reserved memory: {torch.cuda.memory_reserved(i) / (1024**3):.2f} GB\")\n",
    "\n",
    "def find_large_tensors():\n",
    "    \"\"\"Finds tensors occupying significant memory on the GPU.\"\"\"\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "                if obj.is_cuda and obj.numel() * obj.element_size() > 1024**2:  # Tensors larger than 1 MB\n",
    "                    print(f\"Tensor: {type(obj)}, size: {obj.size()}, device: {obj.device}, memory: {obj.numel() * obj.element_size() / (1024**2):.2f} MB\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Print GPU memory usage\n",
    "print_gpu_memory_usage()\n",
    "\n",
    "# Find large tensors\n",
    "find_large_tensors()"
   ],
   "id": "3e8c9f5ed3983923",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0:\n",
      "  Total memory: 15.88 GB\n",
      "  Allocated memory: 11.32 GB\n",
      "  Reserved memory: 13.63 GB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([128256, 2048]), device: cuda:0, memory: 1002.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 16.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 16.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 16.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 16.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 16.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 16.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 16.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 16.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 16.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 16.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 16.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 16.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 16.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 16.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 16.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 16.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 16.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 16.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 16.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 16.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 16.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 16.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 16.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 16.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 16.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 16.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 16.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 16.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 16.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 16.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 16.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 16.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 64.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.Tensor'>, size: torch.Size([2048, 256]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([128256, 2048]), device: cuda:0, memory: 501.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([128256, 2048]), device: cuda:0, memory: 501.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([512, 2048]), device: cuda:0, memory: 2.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 2048]), device: cuda:0, memory: 8.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 2048]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([2048, 8192]), device: cuda:0, memory: 32.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([8192, 256]), device: cuda:0, memory: 4.00 MB\n",
      "Tensor: <class 'torch.nn.parameter.Parameter'>, size: torch.Size([256, 8192]), device: cuda:0, memory: 4.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishabh-project/miniconda3/envs/LLM-RL4Rec-conda-env/lib/python3.12/site-packages/torch/__init__.py:1021: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(obj, torch.Tensor)\n",
      "/tmp/ipykernel_1026969/3732093580.py:16: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "os.environ[\"HUGGINGFACE_API_KEY\"] = \"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\"\n",
    "from packages.evaluation import read_embeddings_from_csv, evaluate_recommendations_for_all_users, distill_top_k_movies\n",
    "from packages.models import map_movies_to_dataset"
   ],
   "id": "b74396db6ffe1b33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# embedding\n",
    "embedding_path = \"/content/drive/MyDrive/CSCI 544 Project/Train SFT /datasets/movie_embeddings.csv\""
   ],
   "id": "ef31e702c174a241"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c62cadbe472bc85a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# work\n",
    "number_of_eval_samples = 10\n",
    "embedding_path = \"/content/drive/MyDrive/CSCI 544 Project/Train SFT /datasets/movie_embeddings.csv\"\n",
    "top_k = 5\n",
    "\n",
    "# iterate over eval dataset and predict\n",
    "metrics = evaluate(dataset['validation'].select(range(number_of_eval_samples)), pipe, top_k, embedding_path, max_new_tokens=256)\n",
    "\n",
    "print(f\"Average Evaluation Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ],
   "id": "9b858f43900bf00b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Issues\n",
    "\n",
    "- Processed Dataset, desired format? Shall we include \"[ ]\"?\n",
    "  - Does every user have five movies to watch?\n",
    "\n",
    "- How to evaluate?\n"
   ],
   "id": "3ea9466fba17ad5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "aa164e8d36822375"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T18:27:10.758937Z",
     "start_time": "2024-12-02T18:26:55.261696Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/merged_peft/final_merged_checkpoint/tokenizer_config.json',\n",
       " './models/merged_peft/final_merged_checkpoint/special_tokens_map.json',\n",
       " './models/merged_peft/final_merged_checkpoint/tokenizer.json')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45,
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel,AutoPeftModelForCausalLM\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Update the path accordingly\n",
    "adapter_dir = './models/sft'\n",
    "output_dir = './models/merged_peft'\n",
    "\n",
    "#model = AutoPeftModelForCausalLM.from_pretrained(adapter_dir, device_map=\"cpu\", torch_dtype=torch.bfloat16)\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(adapter_dir, device_map=\"cuda\", torch_dtype=torch.bfloat16)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_dir)\n",
    "\n",
    "output_merged_dir = os.path.join(output_dir, \"final_merged_checkpoint\")\n",
    "model.save_pretrained(output_merged_dir, safe_serialization=True)\n",
    "tokenizer.save_pretrained(output_merged_dir)"
   ],
   "id": "7809bf11343a199"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T00:25:00.751240Z",
     "start_time": "2024-12-15T00:24:59.415089Z"
    }
   },
   "cell_type": "code",
   "source": "import pandas as pd",
   "id": "3c0db6516e56cd9b",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T00:36:07.717966Z",
     "start_time": "2024-12-15T00:36:02.333083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the paths to the dataset files\n",
    "ratings_path = 'ml-1m/ratings.dat'\n",
    "movies_path = 'ml-1m/movies.dat'\n",
    "users_path = 'ml-1m/users.dat'\n",
    "\n",
    "# Read the ratings data\n",
    "ratings = pd.read_csv('./dataset/'+ratings_path, sep='::', engine='python',\n",
    "                      names=['UserID', 'MovieID', 'Rating', 'Timestamp'])\n",
    "\n",
    "# Read the movies data\n",
    "movies = pd.read_csv('./dataset/'+movies_path, sep='::', engine='python', encoding='latin-1',\n",
    "                     names=['MovieID', 'Title', 'Genres'])\n",
    "\n",
    "# Read the users data\n",
    "users = pd.read_csv('./dataset/'+users_path, sep='::', engine='python',\n",
    "                    names=['UserID', 'Gender', 'Age', 'Occupation', 'Zip-code'])"
   ],
   "id": "4c32f6d5c7e0536",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T00:36:23.447617Z",
     "start_time": "2024-12-15T00:36:23.432589Z"
    }
   },
   "cell_type": "code",
   "source": "ratings.head()",
   "id": "bb3a7db37fbdf0fd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   UserID  MovieID  Rating  Timestamp\n",
       "0       1     1193       5  978300760\n",
       "1       1      661       3  978302109\n",
       "2       1      914       3  978301968\n",
       "3       1     3408       4  978300275\n",
       "4       1     2355       5  978824291"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T00:36:11.168801Z",
     "start_time": "2024-12-15T00:36:11.154317Z"
    }
   },
   "cell_type": "code",
   "source": "movies.head()",
   "id": "9336e4bdae772263",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   MovieID                               Title                        Genres\n",
       "0        1                    Toy Story (1995)   Animation|Children's|Comedy\n",
       "1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
       "2        3             Grumpier Old Men (1995)                Comedy|Romance\n",
       "3        4            Waiting to Exhale (1995)                  Comedy|Drama\n",
       "4        5  Father of the Bride Part II (1995)                        Comedy"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T01:17:55.734753Z",
     "start_time": "2024-12-15T01:17:55.632558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "movies_df = movies.copy()\n",
    "# Step 1: Extract unique genres\n",
    "unique_genres = set()\n",
    "movies_df[\"Genres\"].str.split('|').apply(unique_genres.update)\n",
    "\n",
    "# Step 2: Create binary columns for each genre\n",
    "for genre in unique_genres:\n",
    "    movies_df[genre] = movies_df[\"Genres\"].str.contains(genre).astype(int)"
   ],
   "id": "41808fb1970654d2",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:15:49.200802Z",
     "start_time": "2024-12-15T02:15:49.192746Z"
    }
   },
   "cell_type": "code",
   "source": "movies_df = movies_df.drop(columns=[\"Genres\"])\n",
   "id": "b28aa0d1615beea6",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:15:50.368016Z",
     "start_time": "2024-12-15T02:15:50.343198Z"
    }
   },
   "cell_type": "code",
   "source": "movies_df.head()",
   "id": "98db20aec89ca0b9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   MovieID                               Title  Documentary  Comedy  Crime  \\\n",
       "0        1                    Toy Story (1995)            0       1      0   \n",
       "1        2                      Jumanji (1995)            0       0      0   \n",
       "2        3             Grumpier Old Men (1995)            0       1      0   \n",
       "3        4            Waiting to Exhale (1995)            0       1      0   \n",
       "4        5  Father of the Bride Part II (1995)            0       1      0   \n",
       "\n",
       "   Adventure  Romance  Thriller  War  Animation  Mystery  Film-Noir  Musical  \\\n",
       "0          0        0         0    0          1        0          0        0   \n",
       "1          1        0         0    0          0        0          0        0   \n",
       "2          0        1         0    0          0        0          0        0   \n",
       "3          0        0         0    0          0        0          0        0   \n",
       "4          0        0         0    0          0        0          0        0   \n",
       "\n",
       "   Horror  Action  Drama  Fantasy  Children's  Sci-Fi  Western  \n",
       "0       0       0      0        0           1       0        0  \n",
       "1       0       0      0        1           1       0        0  \n",
       "2       0       0      0        0           0       0        0  \n",
       "3       0       0      1        0           0       0        0  \n",
       "4       0       0      0        0           0       0        0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Film-Noir</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Action</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Children's</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:17:26.379612Z",
     "start_time": "2024-12-15T02:17:26.372167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "movies_df = movies_df.rename(columns = {\n",
    "    \"MovieID\": \"movie_id\",\n",
    "    \"Title\": \"movie_title\",\n",
    "})"
   ],
   "id": "d477d3322cace6f5",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:17:31.681969Z",
     "start_time": "2024-12-15T02:17:31.650007Z"
    }
   },
   "cell_type": "code",
   "source": "movies_df.head()",
   "id": "75d8bfaed370775b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   movie_id                         movie_title  Documentary  Comedy  Crime  \\\n",
       "0         1                    Toy Story (1995)            0       1      0   \n",
       "1         2                      Jumanji (1995)            0       0      0   \n",
       "2         3             Grumpier Old Men (1995)            0       1      0   \n",
       "3         4            Waiting to Exhale (1995)            0       1      0   \n",
       "4         5  Father of the Bride Part II (1995)            0       1      0   \n",
       "\n",
       "   Adventure  Romance  Thriller  War  Animation  Mystery  Film-Noir  Musical  \\\n",
       "0          0        0         0    0          1        0          0        0   \n",
       "1          1        0         0    0          0        0          0        0   \n",
       "2          0        1         0    0          0        0          0        0   \n",
       "3          0        0         0    0          0        0          0        0   \n",
       "4          0        0         0    0          0        0          0        0   \n",
       "\n",
       "   Horror  Action  Drama  Fantasy  Children's  Sci-Fi  Western  \n",
       "0       0       0      0        0           1       0        0  \n",
       "1       0       0      0        1           1       0        0  \n",
       "2       0       0      0        0           0       0        0  \n",
       "3       0       0      1        0           0       0        0  \n",
       "4       0       0      0        0           0       0        0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>movie_title</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Film-Noir</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Action</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Children's</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T01:38:31.232512Z",
     "start_time": "2024-12-15T01:38:31.224767Z"
    }
   },
   "cell_type": "code",
   "source": "print(users.head())",
   "id": "981a9f67367229d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   UserID Gender  Age  Occupation Zip-code\n",
      "0       1      F    1          10    48067\n",
      "1       2      M   56          16    70072\n",
      "2       3      M   25          15    55117\n",
      "3       4      M   45           7    02460\n",
      "4       5      M   25          20    55455\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T01:52:11.040195Z",
     "start_time": "2024-12-15T01:52:11.034121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Age mapping\n",
    "age_mapping = {\n",
    "    1: \"Under 18\",\n",
    "    18: \"18-24\",\n",
    "    25: \"25-34\",\n",
    "    35: \"35-44\",\n",
    "    45: \"45-49\",\n",
    "    50: \"50-55\",\n",
    "    56: \"56+\"\n",
    "}\n",
    "\n",
    "# Occupation mapping\n",
    "occupation_mapping = {\n",
    "    0: \"other\",\n",
    "    1: \"academic/educator\",\n",
    "    2: \"artist\",\n",
    "    3: \"clerical/admin\",\n",
    "    4: \"college/grad student\",\n",
    "    5: \"customer service\",\n",
    "    6: \"doctor/health care\",\n",
    "    7: \"executive/managerial\",\n",
    "    8: \"farmer\",\n",
    "    9: \"homemaker\",\n",
    "    10: \"K-12 student\",\n",
    "    11: \"lawyer\",\n",
    "    12: \"programmer\",\n",
    "    13: \"retired\",\n",
    "    14: \"sales/marketing\",\n",
    "    15: \"scientist\",\n",
    "    16: \"self-employed\",\n",
    "    17: \"technician/engineer\",\n",
    "    18: \"tradesman/craftsman\",\n",
    "    19: \"unemployed\",\n",
    "    20: \"writer\"\n",
    "}"
   ],
   "id": "38afd5a56d926c5",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T01:52:30.984985Z",
     "start_time": "2024-12-15T01:52:30.979710Z"
    }
   },
   "cell_type": "code",
   "source": "users_df = users.copy()",
   "id": "447589e3ab4d951",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T01:53:07.902469Z",
     "start_time": "2024-12-15T01:53:07.887327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Map Age and Occupation columns\n",
    "users_df[\"Age Group\"] = users_df[\"Age\"].map(age_mapping)\n",
    "users_df[\"Occupation Name\"] = users_df[\"Occupation\"].map(occupation_mapping)\n",
    "\n",
    "# Drop original Age and Occupation columns if desired\n",
    "users_df = users_df.drop(columns=[\"Age\", \"Occupation\"])"
   ],
   "id": "de3f75868c1a4b32",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T01:53:14.852499Z",
     "start_time": "2024-12-15T01:53:14.840594Z"
    }
   },
   "cell_type": "code",
   "source": "users_df.head()",
   "id": "69bb6f600a8f7e73",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   UserID Gender Zip-code Age Group       Occupation Name\n",
       "0       1      F    48067  Under 18          K-12 student\n",
       "1       2      M    70072       56+         self-employed\n",
       "2       3      M    55117     25-34             scientist\n",
       "3       4      M    02460     45-49  executive/managerial\n",
       "4       5      M    55455     25-34                writer"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Zip-code</th>\n",
       "      <th>Age Group</th>\n",
       "      <th>Occupation Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>48067</td>\n",
       "      <td>Under 18</td>\n",
       "      <td>K-12 student</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>70072</td>\n",
       "      <td>56+</td>\n",
       "      <td>self-employed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>55117</td>\n",
       "      <td>25-34</td>\n",
       "      <td>scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>02460</td>\n",
       "      <td>45-49</td>\n",
       "      <td>executive/managerial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>55455</td>\n",
       "      <td>25-34</td>\n",
       "      <td>writer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:20:37.030475Z",
     "start_time": "2024-12-15T02:20:37.022300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "users_df = users_df.rename(columns = {\n",
    "    \"UserID\": \"user_id\",\n",
    "    \"Gender\": \"gender\",\n",
    "    \"Age Group\": \"age\",\n",
    "    \"Occupation Name\": \"occupation\",\n",
    "    \"Zip-code\": \"zip_code\",\n",
    "})"
   ],
   "id": "7480d4edf9a4cba1",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:20:44.403608Z",
     "start_time": "2024-12-15T02:20:44.386395Z"
    }
   },
   "cell_type": "code",
   "source": "users_df.head()",
   "id": "381ef88a4b79def5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   user_id gender zip_code       age            occupation\n",
       "0        1      F    48067  Under 18          K-12 student\n",
       "1        2      M    70072       56+         self-employed\n",
       "2        3      M    55117     25-34             scientist\n",
       "3        4      M    02460     45-49  executive/managerial\n",
       "4        5      M    55455     25-34                writer"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>48067</td>\n",
       "      <td>Under 18</td>\n",
       "      <td>K-12 student</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>70072</td>\n",
       "      <td>56+</td>\n",
       "      <td>self-employed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>55117</td>\n",
       "      <td>25-34</td>\n",
       "      <td>scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>02460</td>\n",
       "      <td>45-49</td>\n",
       "      <td>executive/managerial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>55455</td>\n",
       "      <td>25-34</td>\n",
       "      <td>writer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T01:58:55.129829Z",
     "start_time": "2024-12-15T01:58:55.115442Z"
    }
   },
   "cell_type": "code",
   "source": "ratings.head()",
   "id": "6bccb633684c7afe",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   UserID  MovieID  Rating  Timestamp\n",
       "0       1     1193       5  978300760\n",
       "1       1      661       3  978302109\n",
       "2       1      914       3  978301968\n",
       "3       1     3408       4  978300275\n",
       "4       1     2355       5  978824291"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:23:07.190310Z",
     "start_time": "2024-12-15T02:23:07.177735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ratings_df = ratings.rename(columns = {\n",
    "    \"UserID\": \"user_id\",\n",
    "    \"MovieID\": \"movie_id\",\n",
    "    \"Rating\": \"rating\",\n",
    "    \"Timestamp\": \"timestamp\",\n",
    "})"
   ],
   "id": "d9fa8898c25fefc",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:25:58.821966Z",
     "start_time": "2024-12-15T02:25:58.761204Z"
    }
   },
   "cell_type": "code",
   "source": "movies_df.to_csv('./dataset/ml-1m/movies.csv', index=False)",
   "id": "61f9489e72e04be1",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:26:21.038424Z",
     "start_time": "2024-12-15T02:26:21.005567Z"
    }
   },
   "cell_type": "code",
   "source": "users_df.to_csv('./dataset/ml-1m/users.csv', index=False)",
   "id": "7065630317211cbc",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:26:40.829906Z",
     "start_time": "2024-12-15T02:26:38.323016Z"
    }
   },
   "cell_type": "code",
   "source": "ratings_df.to_csv('./dataset/ml-1m/ratings.csv', index=False)",
   "id": "e7942ea995c4e3d7",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:35:03.238556Z",
     "start_time": "2024-12-15T02:35:03.195272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('./dataset/ml-1m/movies.csv')\n",
    "df.head()"
   ],
   "id": "da12e5d4accdc32f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   movie_id                         movie_title  Documentary  Comedy  Crime  \\\n",
       "0         1                    Toy Story (1995)            0       1      0   \n",
       "1         2                      Jumanji (1995)            0       0      0   \n",
       "2         3             Grumpier Old Men (1995)            0       1      0   \n",
       "3         4            Waiting to Exhale (1995)            0       1      0   \n",
       "4         5  Father of the Bride Part II (1995)            0       1      0   \n",
       "\n",
       "   Adventure  Romance  Thriller  War  Animation  Mystery  Film-Noir  Musical  \\\n",
       "0          0        0         0    0          1        0          0        0   \n",
       "1          1        0         0    0          0        0          0        0   \n",
       "2          0        1         0    0          0        0          0        0   \n",
       "3          0        0         0    0          0        0          0        0   \n",
       "4          0        0         0    0          0        0          0        0   \n",
       "\n",
       "   Horror  Action  Drama  Fantasy  Children's  Sci-Fi  Western  \n",
       "0       0       0      0        0           1       0        0  \n",
       "1       0       0      0        1           1       0        0  \n",
       "2       0       0      0        0           0       0        0  \n",
       "3       0       0      1        0           0       0        0  \n",
       "4       0       0      0        0           0       0        0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>movie_title</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Film-Noir</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Action</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Children's</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "140e9001d4998b37"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "b8b2dc28775842638ae313fdbf4fc27c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1d135e557f0a4702bc4219e6fde6e166",
       "IPY_MODEL_939621821f234470a5dfa33de369488d",
       "IPY_MODEL_99ec9d1f5ac04f17bcfa694380c54d7d"
      ],
      "layout": "IPY_MODEL_1530790544b441a99ec79d5e4f7843d1"
     }
    },
    "1d135e557f0a4702bc4219e6fde6e166": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3cc8a6c1bd8b4285967f601a20e85c72",
      "placeholder": "​",
      "style": "IPY_MODEL_fe1b70a9911f4271a9648995b23791be",
      "value": "modules.json: 100%"
     }
    },
    "939621821f234470a5dfa33de369488d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b2b12920003943c6bd3696e0e5e2a3b4",
      "max": 349,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7f02196390e340f48f130f5bb359b48d",
      "value": 349
     }
    },
    "99ec9d1f5ac04f17bcfa694380c54d7d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_27e2516be21548bb86f19347a51b9aa0",
      "placeholder": "​",
      "style": "IPY_MODEL_9f1223d01ab74b3fb234ca996d7effa2",
      "value": " 349/349 [00:00&lt;00:00, 4.70kB/s]"
     }
    },
    "1530790544b441a99ec79d5e4f7843d1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3cc8a6c1bd8b4285967f601a20e85c72": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe1b70a9911f4271a9648995b23791be": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b2b12920003943c6bd3696e0e5e2a3b4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f02196390e340f48f130f5bb359b48d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "27e2516be21548bb86f19347a51b9aa0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f1223d01ab74b3fb234ca996d7effa2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5b5a8a13c6a6477ca276643df410fccb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8f7c27317d734a059ffaf7e147a41601",
       "IPY_MODEL_5f20be13baad45bb816b924ab7287a50",
       "IPY_MODEL_3469f2ab48c841b1986ef6d4198614c1"
      ],
      "layout": "IPY_MODEL_2eaeb22c1d124134bd17bccb84bce106"
     }
    },
    "8f7c27317d734a059ffaf7e147a41601": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c7d200cb8f46469290c5aed519206d74",
      "placeholder": "​",
      "style": "IPY_MODEL_8c1f93973d054889991a25bcd927dbd6",
      "value": "config_sentence_transformers.json: 100%"
     }
    },
    "5f20be13baad45bb816b924ab7287a50": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b1253959c2d45548f141129ab51b358",
      "max": 116,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d05eeaa224e3477191a431d7b340bd26",
      "value": 116
     }
    },
    "3469f2ab48c841b1986ef6d4198614c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e094b2eabaf546598ff3ed36f93825f8",
      "placeholder": "​",
      "style": "IPY_MODEL_7b0a187c44074ebc8676cf746f5bfeaa",
      "value": " 116/116 [00:00&lt;00:00, 2.13kB/s]"
     }
    },
    "2eaeb22c1d124134bd17bccb84bce106": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c7d200cb8f46469290c5aed519206d74": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c1f93973d054889991a25bcd927dbd6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4b1253959c2d45548f141129ab51b358": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d05eeaa224e3477191a431d7b340bd26": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e094b2eabaf546598ff3ed36f93825f8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b0a187c44074ebc8676cf746f5bfeaa": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "034aa3b62a6d4207aa3815a11df25666": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a265e6bbdc5942c5bfb814bd6995f6db",
       "IPY_MODEL_56d073c6029b4a65b85289a7cc6aa649",
       "IPY_MODEL_020617d778a24602b15563c368a33002"
      ],
      "layout": "IPY_MODEL_f070d1d85aa043c7be12f88b448f318b"
     }
    },
    "a265e6bbdc5942c5bfb814bd6995f6db": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8a07cbf6893c477dbe7986446b53d409",
      "placeholder": "​",
      "style": "IPY_MODEL_54c95bca20dd47d4bb16a4610723244d",
      "value": "README.md: 100%"
     }
    },
    "56d073c6029b4a65b85289a7cc6aa649": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ebcf306a47034809a18acb41ec6502d4",
      "max": 10659,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_12bb581cc5b849f48f684cd318f433cc",
      "value": 10659
     }
    },
    "020617d778a24602b15563c368a33002": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_29ff292c76df43df9ca0353d0a16d938",
      "placeholder": "​",
      "style": "IPY_MODEL_45ecb37b4f124744a6754b287b11e787",
      "value": " 10.7k/10.7k [00:00&lt;00:00, 490kB/s]"
     }
    },
    "f070d1d85aa043c7be12f88b448f318b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a07cbf6893c477dbe7986446b53d409": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54c95bca20dd47d4bb16a4610723244d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ebcf306a47034809a18acb41ec6502d4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "12bb581cc5b849f48f684cd318f433cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "29ff292c76df43df9ca0353d0a16d938": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45ecb37b4f124744a6754b287b11e787": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "63ddc046862640f8a4d5991cad2cc69d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_41aae5540c70445794304e5c79a58bc8",
       "IPY_MODEL_3ff75206e6b04782892a214225f7844d",
       "IPY_MODEL_d918d59379914be899c2e0cab55aa06b"
      ],
      "layout": "IPY_MODEL_2c23e9c0b9ab401c8736de204ee17a8c"
     }
    },
    "41aae5540c70445794304e5c79a58bc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9e5d62f2e8af48e68d357519aa575151",
      "placeholder": "​",
      "style": "IPY_MODEL_dbfe164ddb394100872aa0dea2b8a44e",
      "value": "sentence_bert_config.json: 100%"
     }
    },
    "3ff75206e6b04782892a214225f7844d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e31a5e5f34ef4e5db078583d217e5524",
      "max": 53,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_56839a7a7a354b8ba9ead4a8f7c07666",
      "value": 53
     }
    },
    "d918d59379914be899c2e0cab55aa06b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a7318aaf8a7b4e69ac4a602243353030",
      "placeholder": "​",
      "style": "IPY_MODEL_513aa837cfc041aaa49aaccc620e7c69",
      "value": " 53.0/53.0 [00:00&lt;00:00, 848B/s]"
     }
    },
    "2c23e9c0b9ab401c8736de204ee17a8c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e5d62f2e8af48e68d357519aa575151": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dbfe164ddb394100872aa0dea2b8a44e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e31a5e5f34ef4e5db078583d217e5524": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56839a7a7a354b8ba9ead4a8f7c07666": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a7318aaf8a7b4e69ac4a602243353030": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "513aa837cfc041aaa49aaccc620e7c69": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f8d3f22b07a94aa5b912a02da691e830": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_08791e43b8ae4014abe303c395bbc1a5",
       "IPY_MODEL_e2295cdddaa34331925efe9b618d87f0",
       "IPY_MODEL_b2e7d380d44a46c395aaa266e067ad3f"
      ],
      "layout": "IPY_MODEL_52c9034a663b47ebbe61ad59c6068e78"
     }
    },
    "08791e43b8ae4014abe303c395bbc1a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2369d8c326524d1fb8c2eec96005a900",
      "placeholder": "​",
      "style": "IPY_MODEL_9379b28a4b984da0a5b2e01beb315dab",
      "value": "config.json: 100%"
     }
    },
    "e2295cdddaa34331925efe9b618d87f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d6d21cf6062849d48638f573b23bd401",
      "max": 612,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_077c2a5ed621497a915bc5d51745e155",
      "value": 612
     }
    },
    "b2e7d380d44a46c395aaa266e067ad3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6b93a028c341455b9d6977cf12bd773b",
      "placeholder": "​",
      "style": "IPY_MODEL_63188ee87905425ab5863554c972eaee",
      "value": " 612/612 [00:00&lt;00:00, 13.6kB/s]"
     }
    },
    "52c9034a663b47ebbe61ad59c6068e78": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2369d8c326524d1fb8c2eec96005a900": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9379b28a4b984da0a5b2e01beb315dab": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d6d21cf6062849d48638f573b23bd401": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "077c2a5ed621497a915bc5d51745e155": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6b93a028c341455b9d6977cf12bd773b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63188ee87905425ab5863554c972eaee": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e01a7c66fbc94cc4aedf88628d3caedc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d4b413e86e294dcdbde9ead2d07cf371",
       "IPY_MODEL_d508726b0a7b4fa599abf48022173e53",
       "IPY_MODEL_0e17539256374e7da3412babc95552eb"
      ],
      "layout": "IPY_MODEL_e5fe97fca92a4ae1aa01574ab436e81a"
     }
    },
    "d4b413e86e294dcdbde9ead2d07cf371": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9383a1681534fe1b9e8106326510ea7",
      "placeholder": "​",
      "style": "IPY_MODEL_be3a618c148747d2864c8cb98108b1f9",
      "value": "model.safetensors: 100%"
     }
    },
    "d508726b0a7b4fa599abf48022173e53": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_20fdd8f69e0a4d4e93e3ba1a5cc75295",
      "max": 90868376,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_284c1643d46e4a329c94c3885b2f8b1c",
      "value": 90868376
     }
    },
    "0e17539256374e7da3412babc95552eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f8264467ab44795837a48aa3126a593",
      "placeholder": "​",
      "style": "IPY_MODEL_2ba1900cb02943ff8a4c98debbdf74f8",
      "value": " 90.9M/90.9M [00:00&lt;00:00, 128MB/s]"
     }
    },
    "e5fe97fca92a4ae1aa01574ab436e81a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e9383a1681534fe1b9e8106326510ea7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be3a618c148747d2864c8cb98108b1f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "20fdd8f69e0a4d4e93e3ba1a5cc75295": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "284c1643d46e4a329c94c3885b2f8b1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1f8264467ab44795837a48aa3126a593": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ba1900cb02943ff8a4c98debbdf74f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "716249ab96414d77b9b54e54dc616b3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5398b74ac0ce40448d1154e80183b4ed",
       "IPY_MODEL_5a6784d7e8104792b5fc1b807f9bbde4",
       "IPY_MODEL_958c1f4411f64f32a73e27ae7a8b1ce6"
      ],
      "layout": "IPY_MODEL_b2d305ae8a6e44d2aca62c7ca2af27b6"
     }
    },
    "5398b74ac0ce40448d1154e80183b4ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6e7d537991de4af3aacff6ebf6c5e577",
      "placeholder": "​",
      "style": "IPY_MODEL_fb5a90300e2346b096f4f6b2abeb9732",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "5a6784d7e8104792b5fc1b807f9bbde4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_62382dcacb92491ca026ff06bdc8799b",
      "max": 350,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_38875afd9be743ab89ec8472a504830f",
      "value": 350
     }
    },
    "958c1f4411f64f32a73e27ae7a8b1ce6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_38a6f5be06814772afbcad3b26893bab",
      "placeholder": "​",
      "style": "IPY_MODEL_d0f53bc8df19466687e28734a7a47315",
      "value": " 350/350 [00:00&lt;00:00, 7.87kB/s]"
     }
    },
    "b2d305ae8a6e44d2aca62c7ca2af27b6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6e7d537991de4af3aacff6ebf6c5e577": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb5a90300e2346b096f4f6b2abeb9732": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "62382dcacb92491ca026ff06bdc8799b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "38875afd9be743ab89ec8472a504830f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "38a6f5be06814772afbcad3b26893bab": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d0f53bc8df19466687e28734a7a47315": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "99bda92f41f647ab910bf2ed32398587": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c8d5bb069dea4286866f047cd3146fac",
       "IPY_MODEL_28921402548f4488b1098c58d61f1bd2",
       "IPY_MODEL_d540bf8973d64bcc806414846dd6abd3"
      ],
      "layout": "IPY_MODEL_92bebe205260432b82084c15f335620c"
     }
    },
    "c8d5bb069dea4286866f047cd3146fac": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa32310ba32d42018a73f6d366895963",
      "placeholder": "​",
      "style": "IPY_MODEL_1cd0862b36e5425aaeeba27aeb5f020e",
      "value": "vocab.txt: 100%"
     }
    },
    "28921402548f4488b1098c58d61f1bd2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e7520572eeaa48d68c9b846d8cf1948a",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a43aa9eceb264dbe84370e8afa54df97",
      "value": 231508
     }
    },
    "d540bf8973d64bcc806414846dd6abd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c97733d86c454360a0999f94b350e63d",
      "placeholder": "​",
      "style": "IPY_MODEL_26602922058349d4b7e3b01ed8382b1a",
      "value": " 232k/232k [00:00&lt;00:00, 3.22MB/s]"
     }
    },
    "92bebe205260432b82084c15f335620c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa32310ba32d42018a73f6d366895963": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1cd0862b36e5425aaeeba27aeb5f020e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e7520572eeaa48d68c9b846d8cf1948a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a43aa9eceb264dbe84370e8afa54df97": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c97733d86c454360a0999f94b350e63d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26602922058349d4b7e3b01ed8382b1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "621e7811fc0c4af0abede5d781596654": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_17af8b1d08234bf6804765798e41a0f7",
       "IPY_MODEL_6c810efe2cda4452a096316ba5dc161f",
       "IPY_MODEL_587d963178744721b12e8f1f603fe924"
      ],
      "layout": "IPY_MODEL_2073a278865947fcb41638573624fab7"
     }
    },
    "17af8b1d08234bf6804765798e41a0f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f4e5058d39945bc8b3378fbe427a183",
      "placeholder": "​",
      "style": "IPY_MODEL_4780b59997b746daa860e5a13506bec5",
      "value": "tokenizer.json: 100%"
     }
    },
    "6c810efe2cda4452a096316ba5dc161f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8625e44661c54dfeab51df9d83f4fffa",
      "max": 466247,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1f0c275fb0ea455ba81a298ccf26182e",
      "value": 466247
     }
    },
    "587d963178744721b12e8f1f603fe924": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5e756a0dae4c48f4b16f526da50a1184",
      "placeholder": "​",
      "style": "IPY_MODEL_675eb966ff4b4ec8a9a356af1f33182b",
      "value": " 466k/466k [00:00&lt;00:00, 9.23MB/s]"
     }
    },
    "2073a278865947fcb41638573624fab7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f4e5058d39945bc8b3378fbe427a183": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4780b59997b746daa860e5a13506bec5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8625e44661c54dfeab51df9d83f4fffa": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f0c275fb0ea455ba81a298ccf26182e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5e756a0dae4c48f4b16f526da50a1184": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "675eb966ff4b4ec8a9a356af1f33182b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d9c1a07fde4544578ebbd3e02bc718a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fc5a15d65f77413db0fcea25265f2c01",
       "IPY_MODEL_0c547c0409f84dd4bd487c4d55fdbd0d",
       "IPY_MODEL_9f06f25a54a64b6ab6c85c1a654bd26b"
      ],
      "layout": "IPY_MODEL_270886b1d60a49c6b2490430315e97e2"
     }
    },
    "fc5a15d65f77413db0fcea25265f2c01": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d1b4c1392ce94d1384b14f094c302c15",
      "placeholder": "​",
      "style": "IPY_MODEL_271f589292b346d4ba7ba3d3dd62822d",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "0c547c0409f84dd4bd487c4d55fdbd0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ea6fdea0daf407e949a9c29a2ded36e",
      "max": 112,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_68c3147c9d8f4e89aee58bfbfb6d4b67",
      "value": 112
     }
    },
    "9f06f25a54a64b6ab6c85c1a654bd26b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bcc46c5407394f2da4252b1a48789377",
      "placeholder": "​",
      "style": "IPY_MODEL_f08c771091d4438f9ff49eb32c9e47f2",
      "value": " 112/112 [00:00&lt;00:00, 2.09kB/s]"
     }
    },
    "270886b1d60a49c6b2490430315e97e2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d1b4c1392ce94d1384b14f094c302c15": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "271f589292b346d4ba7ba3d3dd62822d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3ea6fdea0daf407e949a9c29a2ded36e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68c3147c9d8f4e89aee58bfbfb6d4b67": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bcc46c5407394f2da4252b1a48789377": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f08c771091d4438f9ff49eb32c9e47f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "047be582b594491688531153b29430ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2ffb325e657c419180e8a8ea70bed19e",
       "IPY_MODEL_3df99fbaad7d492ea4df175dbd70b6f8",
       "IPY_MODEL_6586a0446a894905909f0f9e9cc24006"
      ],
      "layout": "IPY_MODEL_12b821f3e5464cb586b1d7d04335a51b"
     }
    },
    "2ffb325e657c419180e8a8ea70bed19e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d80e01eaf32c4e45a7ac2eaea3af1691",
      "placeholder": "​",
      "style": "IPY_MODEL_9d31458d85434a5c9ce117d7caf2adc2",
      "value": "1_Pooling/config.json: 100%"
     }
    },
    "3df99fbaad7d492ea4df175dbd70b6f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ec1b615c701349098960b287f2ebb991",
      "max": 190,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_17b621ae19fa481dba341f4d2fbb8426",
      "value": 190
     }
    },
    "6586a0446a894905909f0f9e9cc24006": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_09f5c95130954c3cb1791e064f563d2a",
      "placeholder": "​",
      "style": "IPY_MODEL_608a13e8e74941d6aa155ee74685847a",
      "value": " 190/190 [00:00&lt;00:00, 2.44kB/s]"
     }
    },
    "12b821f3e5464cb586b1d7d04335a51b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d80e01eaf32c4e45a7ac2eaea3af1691": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9d31458d85434a5c9ce117d7caf2adc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ec1b615c701349098960b287f2ebb991": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "17b621ae19fa481dba341f4d2fbb8426": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "09f5c95130954c3cb1791e064f563d2a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "608a13e8e74941d6aa155ee74685847a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "43cd6f86ec154a579f91e54bbce5f9b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cdfc88f17c4046c593e1015bdd003988",
       "IPY_MODEL_1d4c19fd1b0845af9a4bc166aa39f1b3",
       "IPY_MODEL_d123628532974ac9a3fb6a13f7c250fc"
      ],
      "layout": "IPY_MODEL_48a20a09ec4842a0b2141c9b37047e81"
     }
    },
    "cdfc88f17c4046c593e1015bdd003988": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7ad37e464940453ab0931cbb355b8589",
      "placeholder": "​",
      "style": "IPY_MODEL_c667a68055324eb5a41789771e1bc615",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "1d4c19fd1b0845af9a4bc166aa39f1b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bdd2e3788e3b4ff5b2e4ed7ac8318510",
      "max": 54528,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1b8bdffeecca482f952ed0204b41eb6c",
      "value": 54528
     }
    },
    "d123628532974ac9a3fb6a13f7c250fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b93ebe93a76440658f55a6b612b4d37a",
      "placeholder": "​",
      "style": "IPY_MODEL_960fec3b8c904e89a9d70c8201197445",
      "value": " 54.5k/54.5k [00:00&lt;00:00, 3.07MB/s]"
     }
    },
    "48a20a09ec4842a0b2141c9b37047e81": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ad37e464940453ab0931cbb355b8589": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c667a68055324eb5a41789771e1bc615": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bdd2e3788e3b4ff5b2e4ed7ac8318510": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b8bdffeecca482f952ed0204b41eb6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b93ebe93a76440658f55a6b612b4d37a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "960fec3b8c904e89a9d70c8201197445": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "71980a4608c54b0bb36389b6da01fe96": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9ec25905a515476f9b3fdd40524f58ed",
       "IPY_MODEL_75ced41494644785a6c75ffc92317d50",
       "IPY_MODEL_79a9fb847d2b4fcd9f8c7cc997d74b49"
      ],
      "layout": "IPY_MODEL_70c4740555c64e39b7099754e19a1ce0"
     }
    },
    "9ec25905a515476f9b3fdd40524f58ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cba9ca72dbbb496f9223bf1f5e941a15",
      "placeholder": "​",
      "style": "IPY_MODEL_94d217274dc8448985ee0c4da658eba6",
      "value": "tokenizer.json: 100%"
     }
    },
    "75ced41494644785a6c75ffc92317d50": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2f810515d5504e7e9b1554194a394eb6",
      "max": 9085657,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9731cd9d640d46bf879961a56ac83c32",
      "value": 9085657
     }
    },
    "79a9fb847d2b4fcd9f8c7cc997d74b49": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13195a0e17fe4dc18413b35db1cea410",
      "placeholder": "​",
      "style": "IPY_MODEL_e22466558cd149938fdad7b407a0a45e",
      "value": " 9.09M/9.09M [00:01&lt;00:00, 8.17MB/s]"
     }
    },
    "70c4740555c64e39b7099754e19a1ce0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cba9ca72dbbb496f9223bf1f5e941a15": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94d217274dc8448985ee0c4da658eba6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2f810515d5504e7e9b1554194a394eb6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9731cd9d640d46bf879961a56ac83c32": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "13195a0e17fe4dc18413b35db1cea410": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e22466558cd149938fdad7b407a0a45e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c5e13b50a39b4e8dac3bac51cb3b5a15": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e58780bb9b364f0a950a5ab7c2830511",
       "IPY_MODEL_d52e86dbae494d1abf8613750e31ffe7",
       "IPY_MODEL_3a447ae69bcf4edca076c1ff3c8e1afd"
      ],
      "layout": "IPY_MODEL_fbedf16a90204718af56b734fd5f07c4"
     }
    },
    "e58780bb9b364f0a950a5ab7c2830511": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f8f483bc3aa34a7f8466d67efca2d08f",
      "placeholder": "​",
      "style": "IPY_MODEL_a958f3b8af1c40a58c0cf475871387f2",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "d52e86dbae494d1abf8613750e31ffe7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9f5417285d33455883149d6254438126",
      "max": 296,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f973103c3a3e4e24a920ebdf4e5990f3",
      "value": 296
     }
    },
    "3a447ae69bcf4edca076c1ff3c8e1afd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c8d5aa424111438fbe9e5e1c66231177",
      "placeholder": "​",
      "style": "IPY_MODEL_b9fa1de6a2e24917a220e175347f9fbf",
      "value": " 296/296 [00:00&lt;00:00, 16.6kB/s]"
     }
    },
    "fbedf16a90204718af56b734fd5f07c4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f8f483bc3aa34a7f8466d67efca2d08f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a958f3b8af1c40a58c0cf475871387f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9f5417285d33455883149d6254438126": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f973103c3a3e4e24a920ebdf4e5990f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c8d5aa424111438fbe9e5e1c66231177": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9fa1de6a2e24917a220e175347f9fbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9282caf99df0458481448a16e284eac3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ebe956a51f5643efb2c5186c3493eef4",
       "IPY_MODEL_08a262e4a0844149bc4d2a95a0a4ed0a",
       "IPY_MODEL_2f9858579f12488e9301395efeaa933e"
      ],
      "layout": "IPY_MODEL_eaa755f60b2f4b2ba7be48b8973b643a"
     }
    },
    "ebe956a51f5643efb2c5186c3493eef4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a0cca3599d1440dbda56b0c9bc317bc",
      "placeholder": "​",
      "style": "IPY_MODEL_f5c2d89a8a5943029f2c57e90cb2b0b7",
      "value": "Map: 100%"
     }
    },
    "08a262e4a0844149bc4d2a95a0a4ed0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c0a7296dd0974798bedade3642a2a21b",
      "max": 611,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ca2559f9a339415cbf5c2ef5083f6b3f",
      "value": 611
     }
    },
    "2f9858579f12488e9301395efeaa933e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e36a15bb43e4142a9d1a4adb8dddfc3",
      "placeholder": "​",
      "style": "IPY_MODEL_2dd5de34d9844af3ae42ae1be8dd4a15",
      "value": " 611/611 [00:00&lt;00:00, 1284.10 examples/s]"
     }
    },
    "eaa755f60b2f4b2ba7be48b8973b643a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6a0cca3599d1440dbda56b0c9bc317bc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f5c2d89a8a5943029f2c57e90cb2b0b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c0a7296dd0974798bedade3642a2a21b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ca2559f9a339415cbf5c2ef5083f6b3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4e36a15bb43e4142a9d1a4adb8dddfc3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2dd5de34d9844af3ae42ae1be8dd4a15": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "917361b29d854c42bd04208b6cb4ff08": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0b26f3dfbf5f4f57973c0e02cfdc37db",
       "IPY_MODEL_604812535c6b4bdd8a113be0ae381081",
       "IPY_MODEL_ac93bdfd7df94a98a1c19867d07c21a3"
      ],
      "layout": "IPY_MODEL_20021227f41d485594fb6cca5c8e4551"
     }
    },
    "0b26f3dfbf5f4f57973c0e02cfdc37db": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_02ee7e98ab90414e91b8eb19764a6e0a",
      "placeholder": "​",
      "style": "IPY_MODEL_8537cb8a476b4d5d823a15d48c3c86bd",
      "value": "Map: 100%"
     }
    },
    "604812535c6b4bdd8a113be0ae381081": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3a8f6791b9eb43c58b6b0936339e7276",
      "max": 611,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_031f0e1c6a3e44e5ba7fa6b706336c5f",
      "value": 611
     }
    },
    "ac93bdfd7df94a98a1c19867d07c21a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_20768a7f981c4b3c90a6364649fac9f8",
      "placeholder": "​",
      "style": "IPY_MODEL_fb2a4012d6334769bc715fe9427f0f78",
      "value": " 611/611 [00:00&lt;00:00, 1462.29 examples/s]"
     }
    },
    "20021227f41d485594fb6cca5c8e4551": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "02ee7e98ab90414e91b8eb19764a6e0a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8537cb8a476b4d5d823a15d48c3c86bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3a8f6791b9eb43c58b6b0936339e7276": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "031f0e1c6a3e44e5ba7fa6b706336c5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "20768a7f981c4b3c90a6364649fac9f8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb2a4012d6334769bc715fe9427f0f78": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
