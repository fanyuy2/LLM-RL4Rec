# -*- coding: utf-8 -*-
"""PPO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13hgw8MHVyhuxZcLC5f6QiZo1EXZF2CHV
"""

import os
os.environ['CUDA_VISIBLE_DEVICES'] = '4,5,6,7'

import json
import datasets
from trl import RewardTrainer, RewardConfig, PPOTrainer, PPOConfig

from peft import LoraConfig, TaskType, get_peft_model
import torch

from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    AutoModelForCausalLM,
    DataCollatorWithPadding
)

with open("/home/zeman/wangmingxi/parsed_preferences.json") as f:
    data = json.load(f)


data_without_prompt = []
for d in data:
    data_without_prompt.append({
        "chosen": d["chosen"],
        "rejected": d["rejected"]
    })
dataset = datasets.Dataset.from_list(data_without_prompt)

dataset

model_name = "meta-llama/Llama-3.2-1B-Instruct"
token = "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"

tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    use_fast=False,
    use_auth_token=token
)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.pad_token_id = 0


def tokenize_function(examples):
    chosen = tokenizer(examples["chosen"], truncation=True, padding=True, max_length=512)
    rejected = tokenizer(examples["rejected"], truncation=True, padding=True, max_length=512)
    chosen = {f"{k}_chosen": v for k, v in chosen.items()}
    rejected = {f"{k}_rejected": v for k, v in rejected.items()}
    return {**chosen, **rejected}

tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=["chosen", "rejected"])

reward_model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=1,
    use_auth_token=token,
    device_map="auto",
)

peft_config = LoraConfig(
    task_type=TaskType.SEQ_CLS,
    inference_mode=False,
    r=8,
    lora_alpha=32,
    lora_dropout=0.1,
)

training_args = RewardConfig(
    output_dir="reward_model_output",
    per_device_train_batch_size=1,
    num_train_epochs=1,
    logging_steps=10,
    save_steps=50,
    learning_rate=5e-5,
    remove_unused_columns=False
)

reward_trainer = RewardTrainer(
    model=reward_model,
    args=training_args,
    train_dataset=tokenized_dataset,
    processing_class=tokenizer,
    peft_config=peft_config
)

print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
print("Training reward model...")
print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
reward_trainer.train()
reward_trainer.save_model("reward_model_lora")

reward_model = AutoModelForSequenceClassification.from_pretrained(
    "reward_model_lora",
    num_labels=1,
    ignore_mismatched_sizes=True
)

# policy_model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=token)
adapter_dir = "/home/zeman/wangmingxi/sft"

policy_model = AutoModelForCausalLM.from_pretrained(
    adapter_dir, device_map="auto", torch_dtype=torch.bfloat16
)


policy_peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    inference_mode=False,
    r=8,
    lora_alpha=32,
    lora_dropout=0.1,
)

policy_model = get_peft_model(policy_model, policy_peft_config)

print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
print("Getting reward...")
print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
ppo_data = []
from tqdm import tqdm
for item in tqdm(data):
    prompt = item["prompt"]
    chosen_response = item["chosen"]  # This is the "winning" response you used to train the reward model.

    # Compute the reward for the (prompt, chosen_response) pair
    combined_input = prompt + " " + chosen_response
    inputs = tokenizer(combined_input, return_tensors="pt", truncation=True, padding=True).to("cuda")

    with torch.no_grad():
        outputs = reward_model(**inputs)
        reward_score = outputs.logits.item()

    # Add this entry to the PPO dataset
    ppo_data.append({
        "query": prompt,
        "response": chosen_response,
        "reward": float(reward_score)
    })

# Now create a Hugging Face Dataset from this list
ppo_dataset = datasets.Dataset.from_list(ppo_data)

def ppo_tokenize_fn(examples):
    # examples["query"] and examples["response"] are lists of queries and responses
    batch_texts = []
    for query, response in zip(examples["query"], examples["response"]):
        batch_texts.append(query + " " + response)

    out = tokenizer(batch_texts, return_tensors="pt", truncation=True, padding=True, max_length=512)
    # print("batched_text", batch_texts)
    # print("out", out)
    # out["input_ids"] = out["input_ids"].squeeze(0)
    out["reward"] = examples["reward"]  # 'reward' should be a list of floats, same length as queries/responses
    return out

ppo_tokenized_dataset = ppo_dataset.map(
    ppo_tokenize_fn,
    batched=True,
    remove_columns=["query", "response"]
)

ppo_config = PPOConfig(
    output_dir="ppo_model_output",
    learning_rate=1e-5,
    batch_size=16,
    eval_strategy="no",
)
ref_policy = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", use_auth_token=token)

value_model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=1,
    device_map="auto",
    use_auth_token=token
)


value_peft_config = LoraConfig(
    task_type=TaskType.SEQ_CLS,
    inference_mode=False,
    r=8,
    lora_alpha=32,
    lora_dropout=0.1,
)
value_model = get_peft_model(value_model, value_peft_config)

# PPO configuration

data_collator = DataCollatorWithPadding(tokenizer)

policy_model.config.padding_token_id = tokenizer.pad_token_id
ref_policy.config.padding_token_id = tokenizer.pad_token_id
value_model.config.padding_token_id = tokenizer.pad_token_id
reward_model.config.padding_token_id = tokenizer.pad_token_id
ppo_config.pad_token_id = tokenizer.pad_token_id
# Now instantiate PPOTrainer with the sequence classification value_model
ppo_trainer = PPOTrainer(
    config=ppo_config,
    policy=policy_model,
    ref_policy=ref_policy,
    value_model=value_model,   # <-- Now a seq. classification model with a score output
    processing_class=tokenizer,
    train_dataset=ppo_tokenized_dataset,
    eval_dataset=ppo_tokenized_dataset,
    reward_model=reward_model,
    data_collator=data_collator
)
# ppo_trainer.generation_config.pad_token_id = 0
print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
print("Training PPO model...")
print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
ppo_trainer.train()

ppo_trainer.policy.save_pretrained("ppo_trained_model")
tokenizer.save_pretrained("ppo_trained_model")
